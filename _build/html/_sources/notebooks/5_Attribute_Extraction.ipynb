{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4abe2d8-1b65-4f6c-95e7-86b0aac222d6",
   "metadata": {},
   "source": [
    "# 5. Basin Attribute Extraction \n",
    "\n",
    "```{figure} img/nalcms_VI.gif\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "North American Land Change Monitoring System {cite}`latifovic2010north` rasters covering Vancouver Island for 2010, 2015, and 2020. \n",
    "```\n",
    "\n",
    "The final step is to capture geospatial information describing the soil, land cover, and terrain of each basin using the polygons we developed in the previous notebook.  First we need to get the data and trim it to the Vancouver Island polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c37eff5-8a65-46f2-a5f9-54ca9608b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the GLHYMPS data\n",
    "import os\n",
    "import time\n",
    "from utilities import retrieve_raster\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from scipy.stats.mstats import gmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1316d23-e92d-43a0-b597-4f2972d66e96",
   "metadata": {},
   "source": [
    "## Download and Clip NALCMS Data\n",
    "\n",
    "North American Coverage of the NALCMS can be downloaded from [North American Land Change Monitoring System (NALCMS)](http://www.cec.org/north-american-land-change-monitoring-system/).  \n",
    "\n",
    "Download the files you want to work with from the link above, and use the steps below to crop the dataset to the region of interest (Vancouver Island)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b85f4271-ef10-4520-abc2-490dbd9b5b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3097683402.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[48], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    nalcms_fpath = # path/to/nalcms_file\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# set the path to the downloaded NALCMS file\n",
    "nalcms_fpath = # path/to/nalcms_file\n",
    "nalcms_raster, nalcms_crs, nalcms_affine = retrieve_raster(nalcms_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89959998-08b4-474e-a147-e5f81c40edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the polygon mask for clipping the NALCMS raster\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "year = 2015\n",
    "region = 'Vancouver_Island'\n",
    "polygon_path = os.path.join(os.getcwd(), f'data/region_polygons/{region}.geojson')\n",
    "region_polygon = gpd.read_file(polygon_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773863b-4f74-4a31-b73a-a3b6f2568d4d",
   "metadata": {},
   "source": [
    "Set the DEM path which was created in notebook 2.  Use the original DEM, not the (pit/depression) filled DEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d8c1e8b-9222-440d-b66a-4efb919d914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be the same path as Notebook 2\n",
    "dem_dir = os.path.join(base_dir, 'notebooks/data/DEM/')\n",
    "dem_fpath = os.path.join(dem_dir, f'{region}_3005.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f91c9fce-28bf-40ad-8522-67fbad338393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for the output geospatial layers\n",
    "output_folder = os.path.join(os.getcwd(), 'data/geospatial_layers')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78aa81-31bc-44ab-a745-107bff438684",
   "metadata": {},
   "source": [
    "We need the polygon mask to have the same CRS as the data source we are clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b446a980-2332-4376-8197-947a75b597ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_polygon = region_polygon.to_crs(nalcms_crs)\n",
    "if not region_polygon.crs == nalcms_crs:\n",
    "    region_polygon = region_polygon.to_crs(nalcms_crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4894d4bc-db3d-45ca-9490-1e92b1748e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dem_path = os.path.join(output_folder, f'NALCMS_{year}_{region}.tif')\n",
    "command = f'gdalwarp -s_srs epsg:{crs} -cutline {polygon_path} -cl Vancouver_Island -crop_to_cutline -multi -of gtiff {nalcms_fpath} {output_dem_path} -wo NUM_THREADS=ALL_CPUS'\n",
    "if not os.path.exists(output_dem_path):\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04754d09-1996-409e-bcf0-c8bb87711f26",
   "metadata": {},
   "source": [
    "## Download and Clip GLHYMPS Data\n",
    "\n",
    "```{figure} img/perm_porosity.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "The GLobal HYdrogeology MaPS (GLHYMPS) {cite}`SP2_TTJNIU_2018` is global coverage of permeability and porosity in vector format.  \n",
    "\n",
    "Download the file from [here](https://aquaknow.jrc.ec.europa.eu/en/content/global-hydrogeology-maps-glhymps-permeability-and-porosity), and use the steps below to clip the data to the region of interest.  \n",
    "\n",
    "Note that this file is large, and it's necessary to use the `mask` feature when opening the file using geopandas. Expect the opening and masking to take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cb27858-8cc8-4010-bed1-57f38b37c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glhymps is in EPSG 4326, ensure the polygon is the same CRS\n",
    "region_polygon = region_polygon.to_crs(4326)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3678a7-8f6f-42f3-8be3-b9980100fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_path = # path to glhymps file\n",
    "gldf = gpd.read_file(glhymps_path, mask=region_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7777e7-865f-4af8-a874-10118d6c1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_output_path = os.path.join(output_folder, f'GLHYMPS_{region}.gpkg')\n",
    "gldf.to_file(glhymps_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7695c51-d456-423f-b452-84baa688f0fc",
   "metadata": {},
   "source": [
    "## Direct Attribute Retrieval\n",
    "\n",
    "Here we load the basin polygon batch files produced in the last notebook, and iterate through polygons to extract attributes as we go.  This method is not the most performant, but the details of the process are hopefully clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bbee3ca-a87e-4685-916b-81aa236b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins_folder = os.path.join(os.getcwd(), 'data/basins/')\n",
    "batches = os.listdir(basins_folder)\n",
    "file = batches[0]\n",
    "basins_df = gpd.read_file(os.path.join(basins_folder, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f7599-b868-441c-93d5-110143d5d31d",
   "metadata": {},
   "source": [
    "\n",
    "**Table: Set of metadata and catchment attributes in the BCUB database derived from USGS 3DEP (DEM), NALCMS (land cover), and GLHYMPS (soil) datasets.**\n",
    "\n",
    "| **Group**  | **Description (BCUB label)** | **Aggregation** | **Units**       |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Metadata   | Pour point (geom)            | -               | decimal deg.$^1$|\n",
    "|            | Basin centroid point (centroid)| -             | decimal deg.    |\n",
    "|            | Land Cover Flag (lulc_check) | -               | binary (0/1)    |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Terrain    | Drainage Area (drainage_area_km2)| at pour point | $km^2$          |\n",
    "|            | Elevation (elevation_m)       | spatial mean    | $m$ above sea level|\n",
    "|            | Terrain Slope (slope_deg)     | spatial mean    | $^\\circ$ (degrees)|\n",
    "|            | Terrain Aspect (aspect_deg)   | circular mean$^2$| $^\\circ$ (degrees)|\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Land Cover$^3$ | Cropland (land_use_crops_frac_<year>) | -     |                   |\n",
    "|            | Forest (land_use_forest_frac_<year>)   | -      |                   |\n",
    "|            | Grassland (land_grass_forest_frac_<year>)| -    |                   |\n",
    "|            | Shrubs (land_use_shrubs_frac_<year>)     | spatial mean| $\\%$ cover    |\n",
    "|            | Snow & Ice (land_use_snow_ice_frac_<year>)| -  |                   |\n",
    "|            | Urban (land_use_urban_frac_<year>)       | -      |                   |\n",
    "|            | Water (land_use_water_frac_<year>)       | -      |                   |\n",
    "|            | Wetland (land_use_wetland_frac_<year>)   | -      |                   |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Soil       | Permeability (permeability_logk_m2)      | geometric mean | $m^2$        |\n",
    "|            | Porosity (porosity_frac)      | spatial mean    | $\\%$ cover     |\n",
    "\n",
    "**Notes**:\n",
    "1.  Geometries are formatted in the WSG84 coordinate reference system.\n",
    "2.  Spatial aspect is expressed in degrees counter-clockwise from the east direction.\n",
    "3.  The <year> suffix specifies the land cover dataset (2010, 2015, or 2020).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254cedd-9d45-4cc4-ae4b-362eb84db74c",
   "metadata": {},
   "source": [
    "## Land Cover Data\n",
    "\n",
    "Below we use LULC to refer to \"land use land cover\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b3db61-9922-4f4a-be68-acbe3693ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attribute_functions import clip_raster_to_basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07bc2816-1f1f-4da4-a493-80bbf146687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lulc_sum(data):\n",
    "    \"\"\"\n",
    "    Check if the sum of pct. land cover sums to 1.\n",
    "    Return value is 1 - sum to correspond with \n",
    "    a more intuitive boolean flag, \n",
    "    i.e. data quality flags are 1 if the flag is raised,\n",
    "    0 of no flag.\n",
    "    \"\"\"\n",
    "    checksum = sum(list(data.values())) \n",
    "    lulc_check = 1-checksum\n",
    "    if abs(lulc_check) >= 0.05:\n",
    "        print(f'   ...checksum failed: {checksum:.3f}')   \n",
    "    return lulc_check\n",
    "\n",
    "\n",
    "def recategorize_lulc(data):    \n",
    "    forest = ('Land_Use_Forest_frac', [1, 2, 3, 4, 5, 6])\n",
    "    shrub = ('Land_Use_Shrubs_frac', [7, 8, 11])\n",
    "    grass = ('Land_Use_Grass_frac', [9, 10, 12, 13, 16])\n",
    "    wetland = ('Land_Use_Wetland_frac', [14])\n",
    "    crop = ('Land_Use_Crops_frac', [15])\n",
    "    urban = ('Land_Use_Urban_frac', [17])\n",
    "    water = ('Land_Use_Water_frac', [18])\n",
    "    snow_ice = ('Land_Use_Snow_Ice_frac', [19])\n",
    "    lulc_dict = {}\n",
    "    for label, p in [forest, shrub, grass, wetland, crop, urban, water, snow_ice]:\n",
    "        prop_vals = round(sum([data[e] if e in data.keys() else 0.0 for e in p]), 2)\n",
    "        lulc_dict[label] = prop_vals\n",
    "    return lulc_dict\n",
    "    \n",
    "\n",
    "def get_value_proportions(data):\n",
    "    # create a dictionary of land cover values by coverage proportion\n",
    "    # assuming raster pixels are equally sized, we can keep the\n",
    "    # raster in geographic coordinates and just count pixel ratios\n",
    "    all_vals = data.data.flatten()\n",
    "    vals = all_vals[~np.isnan(all_vals)]\n",
    "    n_pts = len(vals)\n",
    "    unique, counts = np.unique(vals, return_counts=True)    \n",
    "    prop_dict = {k: 1.0*v/n_pts for k, v in zip(unique, counts)}\n",
    "\n",
    "    if 15 in prop_dict.keys():        \n",
    "        if prop_dict[15] > 0.01:\n",
    "            print(prop_dict)\n",
    "            \n",
    "    prop_dict = recategorize_lulc(prop_dict)\n",
    "    return prop_dict    \n",
    "\n",
    "\n",
    "def process_lulc(basin_geom, nalcms_raster):\n",
    "    # polygon = basin_polygon.to_crs(nalcms_crs)\n",
    "    # assert polygon.crs == nalcms.rio.crs\n",
    "    basin_id = basin_geom['VALUE'].values[0]\n",
    "    raster_loaded, lu_raster_clipped = clip_raster_to_basin(basin_geom, nalcms_raster)\n",
    "    # checksum verifies proportions sum to 1\n",
    "    prop_dict = get_value_proportions(lu_raster_clipped)\n",
    "    lulc_check = check_lulc_sum(prop_dict)\n",
    "    prop_dict['lulc_check'] = lulc_check\n",
    "    return pd.DataFrame(prop_dict, index=[basin_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5780b844-47fd-4135-ab90-2fd9536a7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_repair_geometries(in_feature):\n",
    "\n",
    "    # avoid changing original geodf\n",
    "    in_feature = in_feature.copy(deep=True)    \n",
    "        \n",
    "    # drop any missing geometries\n",
    "    in_feature = in_feature[~(in_feature.is_empty)]\n",
    "    \n",
    "    # Repair broken geometries\n",
    "    for index, row in in_feature.iterrows(): # Looping over all polygons\n",
    "        if row['geometry'].is_valid:\n",
    "            next\n",
    "        else:\n",
    "            fix = make_valid(row['geometry'])\n",
    "            try:\n",
    "                in_feature.loc[[index],'geometry'] =  fix # issue with Poly > Multipolygon\n",
    "            except ValueError:\n",
    "                in_feature.loc[[index],'geometry'] =  in_feature.loc[[index], 'geometry'].buffer(0)\n",
    "    return in_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34bd43b2-7593-4061-b8aa-55ed98bdbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basin_elevation(clipped_raster):\n",
    "    # evaluate masked raster data\n",
    "    values = clipped_raster.data.flatten()\n",
    "    mean_val = np.nanmean(values)\n",
    "    median_val = np.nanmedian(values)\n",
    "    min_val = np.nanmin(values)\n",
    "    max_val = np.nanmax(values)\n",
    "    return mean_val, median_val, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "869f6ae3-6ebc-498f-abf6-413817e9474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soil_properties(merged, col):\n",
    "    # dissolve polygons by unique parameter values\n",
    "    geometries = check_and_repair_geometries(merged)\n",
    "\n",
    "    df = geometries[[col, 'geometry']].copy().dissolve(by=col, aggfunc='first')\n",
    "    df[col] = df.index.values\n",
    "    # re-sum all shape areas\n",
    "    df['Shape_Area'] = df.geometry.area\n",
    "    # calculuate area fractions of each unique parameter value\n",
    "    df['area_frac'] = df['Shape_Area'] / df['Shape_Area'].sum()\n",
    "    # check that the total area fraction = 1\n",
    "    total = round(df['area_frac'].sum(), 1)\n",
    "    sum_check = total == 1.0\n",
    "    if not sum_check:\n",
    "        print(f'    Area proportions do not sum to 1: {total:.2f}')\n",
    "        if np.isnan(total):\n",
    "            return np.nan\n",
    "        elif total < 0.9:\n",
    "            return np.nan\n",
    "    \n",
    "    # area_weighted_vals = df['area_frac'] * df[col]\n",
    "    if 'Permeability' in col:\n",
    "        # calculate geometric mean\n",
    "        # here we change the sign (all permeability values are negative)\n",
    "        # and add it back at the end by multiplying by -1 \n",
    "        # otherwise the function tries to take the log of negative values\n",
    "        return gmean(np.abs(df[col]), weights=df['area_frac']) * -1\n",
    "    else:\n",
    "        # calculate area-weighted arithmetic mean\n",
    "        return (df['area_frac'] * df[col]).sum()\n",
    "    \n",
    "\n",
    "def process_glhymps(basin_geom, fpath):\n",
    "    # import soil layer with polygon mask (both in 4326)\n",
    "    basin_geom = basin_geom.to_crs(4326)\n",
    "    # returns INTERSECTION\n",
    "    gdf = gpd.read_file(fpath, mask=basin_geom)\n",
    "    # now clip precisely to the basin polygon bounds\n",
    "    merged = gpd.clip(gdf, mask=basin_geom)\n",
    "    # now reproject to minimize spatial distortion\n",
    "    merged = merged.to_crs(3005)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73afe944-3d0f-48c7-a2a5-bdda5f125eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit(nopython=True)\n",
    "def process_slope_and_aspect(E, el_px, resolution, shape):\n",
    "    # resolution = E.rio.resolution()\n",
    "    # shape = E.rio.shape\n",
    "    # note, distances are not meaningful in EPSG 4326\n",
    "    # note, we can either do a costly reprojection of the dem\n",
    "    # or just use the approximate resolution of 90x90m\n",
    "    # dx, dy = 90, 90# resolution\n",
    "    dx, dy = resolution\n",
    "    # print(resolution)\n",
    "    # print(asdfd)\n",
    "    # dx, dy = 90, 90\n",
    "    S, A = np.empty_like(E), np.empty_like(E)\n",
    "    S[:] = np.nan # track slope (in degrees)\n",
    "    A[:] = np.nan # track aspect (in degrees)\n",
    "    # tot_p, tot_q = 0, 0\n",
    "    for i, j in el_px:\n",
    "        if (i == 0) | (j == 0) | (i == shape[0]) | (j == shape[1]):\n",
    "            continue\n",
    "            \n",
    "        E_w = E[i-1:i+2, j-1:j+2]\n",
    "\n",
    "        if E_w.shape != (3,3):\n",
    "            continue\n",
    "\n",
    "        a = E_w[0,0]\n",
    "        b = E_w[1,0]\n",
    "        c = E_w[2,0]\n",
    "        d = E_w[0,1]\n",
    "        f = E_w[2,1]\n",
    "        g = E_w[0,2]\n",
    "        h = E_w[1,2]\n",
    "        # skip i and j because they're already used\n",
    "        k = E_w[2,2]  \n",
    "\n",
    "        all_vals = np.array([a, b, c, d, f, g, h, k])\n",
    "\n",
    "        val_check = np.isfinite(all_vals)\n",
    "\n",
    "        if np.all(val_check):\n",
    "            p = ((c + 2*f + k) - (a + 2*d + g)) / (8 * abs(dx))\n",
    "            q = ((c + 2*b + a) - (k + 2*h + g)) / (8 * abs(dy))\n",
    "            cell_slope = np.sqrt(p*p + q*q)\n",
    "            S[i, j] = (180 / np.pi) * np.arctan(cell_slope)\n",
    "            A[i, j] = (180.0 / np.pi) * np.arctan2(q, p)\n",
    "\n",
    "    return S, A\n",
    "\n",
    "\n",
    "def calculate_circular_mean_aspect(a):\n",
    "    \"\"\"\n",
    "    From RavenPy:\n",
    "    https://github.com/CSHS-CWRA/RavenPy/blob/1b167749cdf5984545f8f79ef7d31246418a3b54/ravenpy/utilities/analysis.py#L118\n",
    "    \"\"\"\n",
    "    angles = a[~np.isnan(a)]\n",
    "    n = len(angles)\n",
    "    sine_mean = np.divide(np.sum(np.sin(np.radians(angles))), n)\n",
    "    cosine_mean = np.divide(np.sum(np.cos(np.radians(angles))), n)\n",
    "    vector_mean = np.arctan2(sine_mean, cosine_mean)\n",
    "    degrees = np.degrees(vector_mean)\n",
    "    if degrees < 0:\n",
    "        return degrees + 360\n",
    "    else:\n",
    "        return degrees\n",
    "\n",
    "\n",
    "def calculate_slope_and_aspect(raster):  \n",
    "    \"\"\"Calculate mean basin slope and aspect \n",
    "    according to Hill (1981).\n",
    "\n",
    "    Args:\n",
    "        clipped_raster (array): dem raster\n",
    "\n",
    "    Returns:\n",
    "        slope, aspect: scalar mean values\n",
    "    \"\"\"\n",
    "    # print(raster.data[0])\n",
    "    # print(raster.rio.crs)\n",
    "    # print(asfd)\n",
    "    # wkt = raster.rio.crs.to_wkt()\n",
    "    # affine = raster.rio.transform()\n",
    "\n",
    "    resolution = raster.rio.resolution()\n",
    "    raster_shape = raster[0].shape\n",
    "\n",
    "#     rdem_clipped = rd.rdarray(\n",
    "#         raster.data[0], \n",
    "#         no_data=raster.rio.nodata, \n",
    "#         projection=wkt, \n",
    "#     )\n",
    "\n",
    "#     rdem_clipped.geotransform = affine.to_gdal()\n",
    "#     rdem_clipped.projection = wkt\n",
    "\n",
    "    # # ts0 = time.time()\n",
    "    # use to check slope -- works out to within 1 degree...\n",
    "    # slope = rd.TerrainAttribute(rdem_clipped, attrib='slope_degrees')\n",
    "    # aspect_deg = rd.TerrainAttribute(rdem_clipped, attrib='aspect')\n",
    "    # # ts2 = time.time()\n",
    "\n",
    "    el_px = np.argwhere(np.isfinite(raster.data[0]))\n",
    "\n",
    "    # print(el_px[:2])\n",
    "    # print(rdem_clipped)\n",
    "    # print(asdfsd)\n",
    "    S, A = process_slope_and_aspect(raster.data[0], el_px, resolution, raster_shape)\n",
    "\n",
    "    mean_slope_deg = np.nanmean(S)\n",
    "    # should be within a hundredth of a degree or so.\n",
    "    # print(f'my slope: {mean_slope_deg:.4f}, rdem: {np.nanmean(slope):.4f}')\n",
    "    mean_aspect_deg = calculate_circular_mean_aspect(A)\n",
    "\n",
    "    return mean_slope_deg, mean_aspect_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9d46121-ceda-45d0-bce8-887d95ecc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2010\n",
    "nalcms_fpath = output_dem_path\n",
    "nalcms, nalcms_crs, nalcms_affine = retrieve_raster(nalcms_fpath)\n",
    "\n",
    "region_dem, dem_crs, dem_affine = retrieve_raster(dem_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1be50b8a-b4b3-4e48-939d-e8d68f05fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test, try on a few samples, note how basins_df is sliced in the for .. statement next \n",
    "# to run the whole batch, remove the [:n_samples] slice\n",
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43830042-2cfa-476a-9562-eda778743a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_basin_data = []\n",
    "t0 = time.time()\n",
    "for i, row in basins_df[:n_samples].iterrows():\n",
    "    basin_data = {}\n",
    "    basin_data['region'] = region\n",
    "    basin_data['id'] = row['VALUE']\n",
    "\n",
    "    basin_polygon = basins_df.iloc[[i]].copy()\n",
    "    \n",
    "    clip_ok, clipped_dem = clip_raster_to_basin(basin_polygon, region_dem)\n",
    "    \n",
    "    \n",
    "    land_cover = process_lulc(basin_polygon, nalcms)\n",
    "    land_cover = land_cover.to_dict('records')[0]\n",
    "    # print('     lulc complate')\n",
    "    basin_data.update(land_cover)\n",
    "    \n",
    "    soil = process_glhymps(basin_polygon, glhymps_path)\n",
    "    porosity = get_soil_properties(soil, 'Porosity')\n",
    "    permeability = get_soil_properties(soil, 'Permeability_no_permafrost')\n",
    "    basin_data['Permeability_logk_m2'] = round(permeability, 2)\n",
    "    basin_data['Porosity_frac'] = round(porosity, 5)\n",
    "    \n",
    "    slope, aspect = calculate_slope_and_aspect(clipped_dem)\n",
    "    # print(f'aspect, slope: {aspect:.1f} {slope:.2f} ')\n",
    "    basin_data['Slope_deg'] = slope\n",
    "    basin_data['Aspect_deg'] = aspect\n",
    "\n",
    "\n",
    "    mean_el, median_el, min_el, max_el = process_basin_elevation(clipped_dem)\n",
    "    basin_data['median_el'] = median_el\n",
    "    basin_data['mean_el'] = mean_el\n",
    "    basin_data['max_el'] = max_el\n",
    "    basin_data['min_el'] = min_el\n",
    "    \n",
    "    all_basin_data.append(basin_data)\n",
    "\n",
    "t1 = time.time()\n",
    "ut = (t1 - t0) / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a43406fd-d1a7-4aeb-bdf1-f60608eb4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 basins in 3s (0.30s/basin)\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame(all_basin_data)\n",
    "print(f'Processed {n_samples} basins in {t1-t0:.0f}s ({ut:.2f}s/basin)')\n",
    "# save the file\n",
    "output_fname = file.replace('.geojson', '.csv')\n",
    "output_file = output.to_csv(os.path.join(base_dir, f'notebooks/data/{output_fname}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cbc40-e11c-4185-a2a3-cf6cb8bdb4de",
   "metadata": {},
   "source": [
    "Median processing time is roughly 1s/basin on a ~2018 Intel i7-8850H CPU @ 2.60GHz.  Processing time is proporti onal to the size of clipped DEM, and using JIT in the `process_slope_and_aspect` function yields ~3X performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02febb16-be82-4dfd-8fc5-f09e69d43a34",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography} \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
