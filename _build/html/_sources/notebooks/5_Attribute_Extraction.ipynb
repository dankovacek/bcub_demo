{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4abe2d8-1b65-4f6c-95e7-86b0aac222d6",
   "metadata": {},
   "source": [
    "# 5. Basin Attribute Extraction \n",
    "\n",
    "```{figure} img/nalcms_VI.gif\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "North American Land Change Monitoring System {cite}`latifovic2010north` rasters covering Vancouver Island for 2010, 2015, and 2020. \n",
    "```\n",
    "\n",
    "The final step is to capture geospatial information describing the soil, land cover, and terrain of each basin using the polygons we developed in the previous notebook.  First we need to get the data and trim it to the Vancouver Island polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c37eff5-8a65-46f2-a5f9-54ca9608b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the GLHYMPS data\n",
    "import os\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from utilities import retrieve_raster\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from scipy.stats.mstats import gmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1316d23-e92d-43a0-b597-4f2972d66e96",
   "metadata": {},
   "source": [
    "## Land Cover\n",
    "\n",
    "### Download and Clip NALCMS Data\n",
    "\n",
    "North American Coverage of the NALCMS can be downloaded from [North American Land Change Monitoring System (NALCMS)](http://www.cec.org/north-american-land-change-monitoring-system/).  \n",
    "\n",
    "Download the files you want to work with from the link above, and use the steps below to crop the dataset to the region of interest (Vancouver Island)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85f4271-ef10-4520-abc2-490dbd9b5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.getcwd())\n",
    "# set the path to the downloaded NALCMS file\n",
    "nalcms_fpath = '/home/danbot/Documents/code/23/bcub/input_data/NALCMS/NA_NALCMS_2010_v2_land_cover_30m.tif'\n",
    "nalcms_raster, nalcms_crs, nalcms_affine = retrieve_raster(nalcms_fpath)\n",
    "if nalcms_crs == None:\n",
    "    nalcms_crs = nalcms_raster.rio.crs.wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa1e7e-ff4b-4b07-b7c8-8dbc4e3db1f7",
   "metadata": {},
   "source": [
    "Import the region polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89959998-08b4-474e-a147-e5f81c40edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the polygon mask for clipping the NALCMS raster\n",
    "year = 2010\n",
    "region = 'Vancouver_Island'\n",
    "polygon_path = os.path.join(os.getcwd(), f'data/region_polygons/{region}.geojson')\n",
    "region_polygon = gpd.read_file(polygon_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8c1e8b-9222-440d-b66a-4efb919d914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be the same path as Notebook 2\n",
    "dem_dir = os.path.join(base_dir, 'notebooks/data/processed_dem/')\n",
    "dem_fpath = os.path.join(dem_dir, f'{region}_3005.tif')\n",
    "region_dem, dem_crs, dem_affine = retrieve_raster(dem_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78aa81-31bc-44ab-a745-107bff438684",
   "metadata": {},
   "source": [
    "We need the polygon mask used to clip the raster to have the same CRS as the raster.\n",
    "\n",
    "```{note}\n",
    "The mask has to be saved as a shp file, geojson doesn't work for some reason with gdalwarp.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80bf363-500a-4466-8d1f-179da9ab98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproj_mask_path = polygon_path.replace('.geojson', '_clipping_mask.shp')\n",
    "if not os.path.exists(reproj_mask_path):\n",
    "    mask = region_polygon.to_crs(nalcms_raster.rio.crs.wkt)\n",
    "    mask.to_file(reproj_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773863b-4f74-4a31-b73a-a3b6f2568d4d",
   "metadata": {},
   "source": [
    "Set the DEM path which was created in notebook 2.  Use the original DEM, not the (pit/depression) filled DEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91c9fce-28bf-40ad-8522-67fbad338393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for the output geospatial layers\n",
    "output_folder = os.path.join(os.getcwd(), 'data/geospatial_layers')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4894d4bc-db3d-45ca-9490-1e92b1748e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_path = os.path.join(output_folder, f'NALCMS_{year}_{region}.tif')\n",
    "command = f\"gdalwarp -s_srs '{nalcms_crs}' -cutline {reproj_mask_path} -crop_to_cutline -multi -of gtiff {nalcms_fpath} {clipped_path} -wo NUM_THREADS=ALL_CPUS\"\n",
    "if not os.path.exists(clipped_path):\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04754d09-1996-409e-bcf0-c8bb87711f26",
   "metadata": {},
   "source": [
    "## Soil Permeability and Porosity\n",
    "\n",
    "### Download and Clip GLHYMPS Data\n",
    "\n",
    "```{figure} img/perm_porosity.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "The GLobal HYdrogeology MaPS (GLHYMPS) {cite}`SP2_TTJNIU_2018` is global coverage of permeability and porosity in vector format.  \n",
    "\n",
    "Download the file from [here](https://aquaknow.jrc.ec.europa.eu/en/content/global-hydrogeology-maps-glhymps-permeability-and-porosity), and use the steps below to clip the data to the region of interest.  \n",
    "\n",
    "Note that this file is large, and it's necessary to use the `mask` feature when opening the file using geopandas. Expect the opening and masking to take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb27858-8cc8-4010-bed1-57f38b37c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glhymps is in EPSG 4326, ensure the polygon is the same CRS\n",
    "region_polygon = region_polygon.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3678a7-8f6f-42f3-8be3-b9980100fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_path = 'data/geospatial_layers/GLHYMPS_Vancouver_Island.gpkg'\n",
    "gldf = gpd.read_file(glhymps_path, mask=region_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd50fd-6354-419c-b08d-4fe58a69adae",
   "metadata": {},
   "source": [
    "## Climate Data\n",
    "\n",
    "### Accessing and Registering for NASA DAYMET Data\n",
    "\n",
    "NASA's DAYMET provides daily surface weather and climatological summaries for North America.  To access and automate the download of DAYMET data, follow these steps:\n",
    "\n",
    "1. **Register**: Before you can download data, you need to [register with ORNL DAAC](https://urs.earthdata.nasa.gov/). \n",
    "   \n",
    "2. **Access the Data**: Once registered, navigate to the [DAYMET Data Collection page](https://daymet.ornl.gov/) where you can explore available data sets.\n",
    "   \n",
    "3. **Automated Download**: For automated data downloads, you can use the DAYMET web services. Detailed instructions and examples for using these services can be found in the [DAYMET documentation](https://daymet.ornl.gov/web_services.html).\n",
    "\n",
    "A listing of all available daily DAYMET spatial time series can be found [here](https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/2129/catalog.html).\n",
    "\n",
    "Available climate variables are as follows:\n",
    "\n",
    "| Variable | Description (units) |\n",
    "|---|---|\n",
    "| tmax | Daily maximum 2-meter air temperature (°C) |\n",
    "| tmin | Daily minimum 2-meter air temperature (°C) |\n",
    "| prcp | Daily total precipitation (mm/day) |\n",
    "| srad | Incident shortwave radiation flux density ($W/m^2$) |\n",
    "| vp | Water vapor pressure (Pa) |\n",
    "| swe | Snow water equivalent ($kg/m^2$) |\n",
    "| dayl | Duration of the daylight period (seconds/day) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ca12a3-89c8-4ced-8b4b-4e35ad50f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the daymet tile index\n",
    "tile_fpath = os.path.join(base_dir, 'notebooks/data/daymet_data/Daymet_v4_Tiles.geojson')\n",
    "dm_tiles = gpd.read_file(tile_fpath)\n",
    "\n",
    "# get the intersection with the region polygon\n",
    "tiles_df = dm_tiles.sjoin(region_polygon)\n",
    "tiles_df = tiles_df.sort_values(by=['Latitude (Min)', 'Longitude (Min)'])\n",
    "tile_rows = tiles_df.groupby('Latitude (Min)')['TileID'].apply(list).tolist()\n",
    "tile_ids = tiles_df['TileID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de209e3d-bc06-4dfd-8d82-153c0b2db2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_tmin.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_tmax.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_prcp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_srad.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_swe.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_swe.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_swe.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12449_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12448_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12447_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_swe.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12628_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_swe.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12627_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_vp.nc: No such file or directory\n",
      "/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET/12626_2022_swe.nc: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "daymet_url_base = 'https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/2130/catalog.html?'\n",
    "daymet_params = ['tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp']\n",
    "years = list(range(1980,2023))\n",
    "daymet_save_path ='/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET'\n",
    "\n",
    "base_command = 'wget -q --show-progress --progress=bar:force --limit-rate=3m https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/2129/tiles/'\n",
    "\n",
    "for yr in years:\n",
    "    batch_commands = []\n",
    "    for param in daymet_params:\n",
    "        for tile in tile_ids:\n",
    "            file = f'{daymet_save_path}/{tile}_{yr}_{param}.nc'\n",
    "            if not os.path.exists(file):\n",
    "                cmd = base_command + f'{yr}/{tile}_{yr}/{param}.nc -O {file}'\n",
    "                batch_commands.append(cmd)\n",
    "\n",
    "# download the files in parallel\n",
    "with mp.Pool() as pl:\n",
    "    pl.map(os.system, batch_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e878dff-36d2-4054-b264-e2e08928ee88",
   "metadata": {},
   "source": [
    "### Process NASA DAYMET Data\n",
    "\n",
    "[Installing CDO](https://code.mpimet.mpg.de/projects/cdo/wiki) is a way to manage the climate data processing where we need to process spatial timeseries for several parameters.\n",
    "\n",
    "To process the NASA DAYMET data spanning from 1980 to 2022 for specific parameters ('tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp') within a set of polygons, we will use the following approach:\n",
    "\n",
    "1. **Data Preparation**: Begin by organizing the .nc (NetCDF) files in an xarray dataset.\n",
    "\n",
    "2. **Polygon Masking**: Convert your polygons into a raster format that matches the spatial resolution and extent of the DAYMET data. Each pixel within the polygon should have a value of 1 (or true), while those outside the polygon should be 0 (or false).\n",
    "\n",
    "3. **Data Extraction**: For each year and parameter:  \n",
    "  a. Load the .nc file using a spatial data processing library like GDAL in Python \n",
    "  b. Multiply the DAYMET raster data by the polygon mask raster. This operation will effectively 'zero out' the data outside of your polygons, leaving you with data values only within the desired regions.  \n",
    "  c. Compute desired statistics for each masked region, such as mean, sum, max, or min, depending on your research objectives.\n",
    "\n",
    "4. **Aggregation**: Once you've extracted the data for all parameters across the years, you can aggregate or analyze the time series data as per your needs, e.g., trend analysis, anomaly detection, or temporal summarization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17d74e8-8789-4354-9a40-1e6e62d72796",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tiles_df.crs == region_polygon.crs\n",
    "reproj_mask_path = polygon_path.replace('.geojson', '_daymet_mask.shp')\n",
    "if not os.path.exists(reproj_mask_path):\n",
    "    mask = region_polygon.to_crs(tiles_df.crs)\n",
    "    mask.to_file(reproj_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cf5d68-7a8e-41c7-9b4f-7ccb8f4a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_netcdf_data(fpath, param, region_mask):\n",
    "    ds = rxr.open_rasterio(fpath, engine=\"rasterio\", mask=region_mask.geometry[0])\n",
    "    crs = ds.rio.crs.to_wkt()\n",
    "    # region_polygon = gpd.read_file(reproj_mask_path).to_crs(crs)\n",
    "    # ds = ds.rio.clip(region_polygon.geometry)\n",
    "    ds = ds.to_dataset(name=param)    \n",
    "    ds = ds.rename_dims({'band': 'dayofyear'}).rename_vars({'band': 'day'})\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13180d8-312f-44f5-a988-020751e3a8a4",
   "metadata": {},
   "source": [
    "For each daymet parameter:\n",
    "* for each year, build a vrt from the tiled .nc files\n",
    "* open the vrt as an xarray dataset and identify the time coordinate as dayofyear\n",
    "* for each year, generate a summary statistic (sum for precip, max for swe, mean for remainder)\n",
    "* concatenate the summarized annual values into a new xarray dataset where the time coordinate is now 'years'\n",
    "* save the resulting dataset as an annual spatial time series in .nc format  \n",
    "    * these new files will be masked with each basin polygon, and some index computed for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f765dda-9b05-4fab-9df9-4dd7a6f25c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for param in daymet_params:\n",
    "    param_folder = f'data/daymet_data/{param}'\n",
    "    region_mask = gpd.read_file(reproj_mask_path)\n",
    "    \n",
    "    if not os.path.exists(param_folder):\n",
    "        os.mkdir(param_folder)\n",
    "    data_arrays = []\n",
    "    temp_files = []\n",
    "    \n",
    "    for year in years:\n",
    "        tile_mosaic = []\n",
    "        # Specify the pattern for your file paths.\n",
    "        file_pattern = f'*_{year}_{param}.nc'\n",
    "        output_fname = f'{year}_{param}.vrt'\n",
    "        output_fpath = os.path.join(param_folder, output_fname)\n",
    "        \n",
    "        # assemble the mosaic\n",
    "        cmd = f'gdalbuildvrt {output_fpath} {daymet_save_path}/{file_pattern}'\n",
    "        if not os.path.exists(output_fpath):\n",
    "            os.system(cmd)\n",
    "            temp_files.append(output_fpath)\n",
    "\n",
    "        # clipped_path = os.path.join(f'data/daymet_data', f'{year}_{param}_clipped.tif')\n",
    "        data = merge_netcdf_data(output_fpath, param, region_mask)\n",
    "        if param in ['prcp']:\n",
    "            spatial_result = data.sum(dim='dayofyear')\n",
    "        elif param == 'swe':\n",
    "            spatial_result = data.max(dim='dayofyear')\n",
    "        else:\n",
    "            spatial_result = data.mean(dim='dayofyear')\n",
    "        \n",
    "        output_rpath = f'{param_folder}/{year}_{param}_mean.tif'\n",
    "        data_arrays.append(spatial_result[param])\n",
    "\n",
    "    ts_dataset = xr.concat(data_arrays, dim=pd.Index(years, name='time'))\n",
    "    ts_dataset.to_netcdf(f'data/daymet_data/{param}_{min(years)}_to_{max(years)}.nc')\n",
    "    print(param)\n",
    "    print(ts_dataset)\n",
    "    print('')\n",
    "    # for f in temp_files:\n",
    "    #     if os.path.exists(f):\n",
    "    #         os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7695c51-d456-423f-b452-84baa688f0fc",
   "metadata": {},
   "source": [
    "## Direct Attribute Retrieval\n",
    "\n",
    "Here we load the basin polygon batch files produced in the last notebook, and iterate through polygons to extract attributes as we go.  This method is not the most performant, but the details of the process are hopefully clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bbee3ca-a87e-4685-916b-81aa236b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins_folder = os.path.join(os.getcwd(), 'data/basins/')\n",
    "batches = os.listdir(basins_folder)\n",
    "file = batches[0]\n",
    "basins_df = gpd.read_file(os.path.join(basins_folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c178ff23-fa83-49d0-93e4-22a92e3d1074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_idx</th>\n",
       "      <th>acc</th>\n",
       "      <th>OUTLET</th>\n",
       "      <th>CONF</th>\n",
       "      <th>ix</th>\n",
       "      <th>jx</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199,4302</td>\n",
       "      <td>22657.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199.0</td>\n",
       "      <td>4302.0</td>\n",
       "      <td>POINT (865049.001 659747.390)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214,4329</td>\n",
       "      <td>21779.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4329.0</td>\n",
       "      <td>POINT (865649.625 659413.710)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214,4330</td>\n",
       "      <td>18179.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4330.0</td>\n",
       "      <td>POINT (865671.870 659413.710)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215,4329</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4329.0</td>\n",
       "      <td>POINT (865649.625 659391.465)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234,4352</td>\n",
       "      <td>11530.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>234.0</td>\n",
       "      <td>4352.0</td>\n",
       "      <td>POINT (866161.268 658968.803)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23331</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (1231963.627 409643.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23332</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (1172768.779 423412.905)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (1171211.606 421989.204)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23334</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (1180866.083 434513.330)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (1179553.608 434780.274)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23336 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cell_idx      acc OUTLET CONF     ix      jx  \\\n",
       "0      199,4302  22657.0      1    1  199.0  4302.0   \n",
       "1      214,4329  21779.0      0    1  214.0  4329.0   \n",
       "2      214,4330  18179.0      0    1  214.0  4330.0   \n",
       "3      215,4329   3591.0      0    1  215.0  4329.0   \n",
       "4      234,4352  11530.0      0    1  234.0  4352.0   \n",
       "...         ...      ...    ...  ...    ...     ...   \n",
       "23331       NaN      NaN    NaN  NaN    NaN     NaN   \n",
       "23332       NaN      NaN    NaN  NaN    NaN     NaN   \n",
       "23333       NaN      NaN    NaN  NaN    NaN     NaN   \n",
       "23334       NaN      NaN    NaN  NaN    NaN     NaN   \n",
       "23335       NaN      NaN    NaN  NaN    NaN     NaN   \n",
       "\n",
       "                             geometry  \n",
       "0       POINT (865049.001 659747.390)  \n",
       "1       POINT (865649.625 659413.710)  \n",
       "2       POINT (865671.870 659413.710)  \n",
       "3       POINT (865649.625 659391.465)  \n",
       "4       POINT (866161.268 658968.803)  \n",
       "...                               ...  \n",
       "23331  POINT (1231963.627 409643.040)  \n",
       "23332  POINT (1172768.779 423412.905)  \n",
       "23333  POINT (1171211.606 421989.204)  \n",
       "23334  POINT (1180866.083 434513.330)  \n",
       "23335  POINT (1179553.608 434780.274)  \n",
       "\n",
       "[23336 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_fpath = f'data/pour_points/Vancouver_Island_pour_points.geojson'\n",
    "ppt_df = gpd.read_file(ppt_fpath)\n",
    "ppt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f7599-b868-441c-93d5-110143d5d31d",
   "metadata": {},
   "source": [
    "\n",
    "**Table: Set of metadata and catchment attributes in the BCUB database derived from USGS 3DEP (DEM), NALCMS (land cover), and GLHYMPS (soil) datasets.**\n",
    "\n",
    "| **Group**  | **Description (BCUB label)** | **Aggregation** | **Units**       |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Metadata   | Pour point (geom)            | -               | decimal deg.$^1$|\n",
    "|            | Basin centroid point (centroid)| -             | decimal deg.    |\n",
    "|            | Land Cover Flag (lulc_check) | -               | binary (0/1)    |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Terrain    | Drainage Area (drainage_area_km2)| at pour point | $km^2$          |\n",
    "|            | Elevation (elevation_m)       | spatial mean    | $m$ above sea level|\n",
    "|            | Terrain Slope (slope_deg)     | spatial mean    | $^\\circ$ (degrees)|\n",
    "|            | Terrain Aspect (aspect_deg)   | circular mean$^2$| $^\\circ$ (degrees)|\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Land Cover$^3$ | Cropland (land_use_crops_frac_<year>) | -     |                   |\n",
    "|            | Forest (land_use_forest_frac_<year>)   | -      |                   |\n",
    "|            | Grassland (land_grass_forest_frac_<year>)| -    |                   |\n",
    "|            | Shrubs (land_use_shrubs_frac_<year>)     | spatial mean| $\\%$ cover    |\n",
    "|            | Snow & Ice (land_use_snow_ice_frac_<year>)| -  |                   |\n",
    "|            | Urban (land_use_urban_frac_<year>)       | -      |                   |\n",
    "|            | Water (land_use_water_frac_<year>)       | -      |                   |\n",
    "|            | Wetland (land_use_wetland_frac_<year>)   | -      |                   |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Soil       | Permeability (permeability_logk_m2)      | geometric mean | $m^2$        |\n",
    "|            | Porosity (porosity_frac)      | spatial mean    | $\\%$ cover     |\n",
    "|------------|------------------------------|-----------------|-----------------|\n",
    "| Climate    | Precipitation (mean_precip_mm)    | sum | $mm$        |\n",
    "|            | Shortwave Radiation (solar_rad_Wm2)   |  spatial mean | $W/m^2$    |\n",
    "|            | Water Vapour Pressure (water_vap_Pa)   |  spatial mean | $Pa$    |\n",
    "|            | Snow Water Equivalent (swe_kgm2) | spatial mean | $kg/m^2$    |\n",
    "|            | Min Temperature (min_temp_C)  |  spatial mean | $C$    |\n",
    "|            | Max Temperature (max_temp_C)  |  spatial mean | $C$    |\n",
    "\n",
    "**Notes**:\n",
    "1.  Geometries are formatted in the WSG84 coordinate reference system.\n",
    "2.  Spatial aspect is expressed in degrees counter-clockwise from the east direction.\n",
    "3.  The <year> suffix specifies the land cover dataset (2010, 2015, or 2020).\n",
    "4.  Specifications on DAYMET data can be found [here](https://daac.ornl.gov/DAYMET/guides/Daymet_Daily_V4.html#:~:text=Daymet%20variables%20include%20the%20following,water%20equivalent%2C%20and%20day%20length.).\n",
    "5.  All climate parameters are mean annual values over 1980-2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254cedd-9d45-4cc4-ae4b-362eb84db74c",
   "metadata": {},
   "source": [
    "## Land Cover Data\n",
    "\n",
    "Land use land cover (LULC) classes are grouped as in Arsenault (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34b3db61-9922-4f4a-be68-acbe3693ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attribute_functions import clip_raster_to_basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07bc2816-1f1f-4da4-a493-80bbf146687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lulc_sum(data):\n",
    "    \"\"\"\n",
    "    Check if the sum of pct. land cover sums to 1.\n",
    "    Return value is 1 - sum to correspond with \n",
    "    a more intuitive boolean flag, \n",
    "    i.e. data quality flags are 1 if the flag is raised,\n",
    "    0 of no flag.\n",
    "    \"\"\"\n",
    "    checksum = sum(list(data.values())) \n",
    "    lulc_check = 1-checksum\n",
    "    if abs(lulc_check) >= 0.05:\n",
    "        print(f'   ...checksum failed: {checksum:.3f}')   \n",
    "    return lulc_check\n",
    "\n",
    "\n",
    "def recategorize_lulc(data):    \n",
    "    forest = ('Land_Use_Forest_frac', [1, 2, 3, 4, 5, 6])\n",
    "    shrub = ('Land_Use_Shrubs_frac', [7, 8, 11])\n",
    "    grass = ('Land_Use_Grass_frac', [9, 10, 12, 13, 16])\n",
    "    wetland = ('Land_Use_Wetland_frac', [14])\n",
    "    crop = ('Land_Use_Crops_frac', [15])\n",
    "    urban = ('Land_Use_Urban_frac', [17])\n",
    "    water = ('Land_Use_Water_frac', [18])\n",
    "    snow_ice = ('Land_Use_Snow_Ice_frac', [19])\n",
    "    lulc_dict = {}\n",
    "    for label, p in [forest, shrub, grass, wetland, crop, urban, water, snow_ice]:\n",
    "        prop_vals = round(sum([data[e] if e in data.keys() else 0.0 for e in p]), 2)\n",
    "        lulc_dict[label] = prop_vals\n",
    "    return lulc_dict\n",
    "    \n",
    "\n",
    "def get_value_proportions(data):\n",
    "    # create a dictionary of land cover values by coverage proportion\n",
    "    # assuming raster pixels are equally sized, we can keep the\n",
    "    # raster in geographic coordinates and just count pixel ratios\n",
    "    all_vals = data.data.flatten()\n",
    "    vals = all_vals[~np.isnan(all_vals)]\n",
    "    n_pts = len(vals)\n",
    "    unique, counts = np.unique(vals, return_counts=True)    \n",
    "    prop_dict = {k: 1.0*v/n_pts for k, v in zip(unique, counts)}\n",
    "\n",
    "    # 15 represents cropland\n",
    "    # if 15 in prop_dict.keys():\n",
    "    #     if prop_dict[15] > 0.01:\n",
    "    #         print(f'Land cover category 15 is found: {prop_dict[15]}%')\n",
    "    #         print(prop_dict)\n",
    "            \n",
    "    prop_dict = recategorize_lulc(prop_dict)\n",
    "    return prop_dict    \n",
    "\n",
    "\n",
    "def process_lulc(basin_geom, nalcms_raster):\n",
    "    # polygon = basin_polygon.to_crs(nalcms_crs)\n",
    "    # assert polygon.crs == nalcms.rio.crs\n",
    "    basin_id = basin_geom['VALUE'].values[0]\n",
    "    if nalcms_raster.rio.crs != basin_geom.crs:\n",
    "        basin_geom = basin_geom.to_crs(nalcms_raster.rio.crs.to_wkt())\n",
    "    raster_loaded, lu_raster_clipped = clip_raster_to_basin(basin_geom, nalcms_raster)\n",
    "    # checksum verifies proportions sum to 1\n",
    "    prop_dict = get_value_proportions(lu_raster_clipped)\n",
    "    lulc_check = check_lulc_sum(prop_dict)\n",
    "    prop_dict['lulc_check'] = lulc_check\n",
    "    return pd.DataFrame(prop_dict, index=[basin_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5780b844-47fd-4135-ab90-2fd9536a7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_repair_geometries(in_feature):\n",
    "\n",
    "    # avoid changing original geodf\n",
    "    in_feature = in_feature.copy(deep=True)    \n",
    "        \n",
    "    # drop any missing geometries\n",
    "    in_feature = in_feature[~(in_feature.is_empty)]\n",
    "    \n",
    "    # Repair broken geometries\n",
    "    for index, row in in_feature.iterrows(): # Looping over all polygons\n",
    "        if row['geometry'].is_valid:\n",
    "            next\n",
    "        else:\n",
    "            fix = make_valid(row['geometry'])\n",
    "            try:\n",
    "                in_feature.loc[[index],'geometry'] =  fix # issue with Poly > Multipolygon\n",
    "            except ValueError:\n",
    "                in_feature.loc[[index],'geometry'] =  in_feature.loc[[index], 'geometry'].buffer(0)\n",
    "    return in_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34bd43b2-7593-4061-b8aa-55ed98bdbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basin_elevation(clipped_raster):\n",
    "    # evaluate masked raster data\n",
    "    values = clipped_raster.data.flatten()\n",
    "    mean_val = np.nanmean(values)\n",
    "    median_val = np.nanmedian(values)\n",
    "    min_val = np.nanmin(values)\n",
    "    max_val = np.nanmax(values)\n",
    "    return mean_val, median_val, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "869f6ae3-6ebc-498f-abf6-413817e9474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soil_properties(merged, col):\n",
    "    # dissolve polygons by unique parameter values\n",
    "    geometries = check_and_repair_geometries(merged)\n",
    "\n",
    "    df = geometries[[col, 'geometry']].copy().dissolve(by=col, aggfunc='first')\n",
    "    df[col] = df.index.values\n",
    "    # re-sum all shape areas\n",
    "    df['Shape_Area'] = df.geometry.area\n",
    "    # calculuate area fractions of each unique parameter value\n",
    "    df['area_frac'] = df['Shape_Area'] / df['Shape_Area'].sum()\n",
    "    # check that the total area fraction = 1\n",
    "    total = round(df['area_frac'].sum(), 1)\n",
    "    sum_check = total == 1.0\n",
    "    if not sum_check:\n",
    "        print(f'    Area proportions do not sum to 1: {total:.2f}')\n",
    "        if np.isnan(total):\n",
    "            return np.nan\n",
    "        elif total < 0.9:\n",
    "            return np.nan\n",
    "    \n",
    "    # area_weighted_vals = df['area_frac'] * df[col]\n",
    "    if 'Permeability' in col:\n",
    "        # calculate geometric mean\n",
    "        # here we change the sign (all permeability values are negative)\n",
    "        # and add it back at the end by multiplying by -1 \n",
    "        # otherwise the function tries to take the log of negative values\n",
    "        return gmean(np.abs(df[col]), weights=df['area_frac']) * -1\n",
    "    else:\n",
    "        # calculate area-weighted arithmetic mean\n",
    "        return (df['area_frac'] * df[col]).sum()\n",
    "    \n",
    "\n",
    "def process_glhymps(basin_geom, fpath):\n",
    "    # import soil layer with polygon mask (both in 4326)\n",
    "    basin_geom = basin_geom.to_crs(4326)\n",
    "    # returns INTERSECTION\n",
    "    gdf = gpd.read_file(fpath, mask=basin_geom)\n",
    "    # now clip precisely to the basin polygon bounds\n",
    "    merged = gpd.clip(gdf, mask=basin_geom)\n",
    "    # now reproject to minimize spatial distortion\n",
    "    merged = merged.to_crs(3005)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73afe944-3d0f-48c7-a2a5-bdda5f125eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def process_slope_and_aspect(E, el_px, resolution, shape):\n",
    "    # resolution = E.rio.resolution()\n",
    "    # shape = E.rio.shape\n",
    "    # note, distances are not meaningful in EPSG 4326\n",
    "    # note, we can either do a costly reprojection of the dem\n",
    "    # or just use the approximate resolution of 90x90m\n",
    "    # dx, dy = 90, 90# resolution\n",
    "    dx, dy = resolution\n",
    "    # print(resolution)\n",
    "    # print(asdfd)\n",
    "    # dx, dy = 90, 90\n",
    "    S, A = np.empty_like(E), np.empty_like(E)\n",
    "    S[:] = np.nan # track slope (in degrees)\n",
    "    A[:] = np.nan # track aspect (in degrees)\n",
    "    # tot_p, tot_q = 0, 0\n",
    "    for i, j in el_px:\n",
    "        if (i == 0) | (j == 0) | (i == shape[0]) | (j == shape[1]):\n",
    "            continue\n",
    "            \n",
    "        E_w = E[i-1:i+2, j-1:j+2]\n",
    "\n",
    "        if E_w.shape != (3,3):\n",
    "            continue\n",
    "\n",
    "        a = E_w[0,0]\n",
    "        b = E_w[1,0]\n",
    "        c = E_w[2,0]\n",
    "        d = E_w[0,1]\n",
    "        f = E_w[2,1]\n",
    "        g = E_w[0,2]\n",
    "        h = E_w[1,2]\n",
    "        # skip i and j because they're already used\n",
    "        k = E_w[2,2]  \n",
    "\n",
    "        all_vals = np.array([a, b, c, d, f, g, h, k])\n",
    "\n",
    "        val_check = np.isfinite(all_vals)\n",
    "\n",
    "        if np.all(val_check):\n",
    "            p = ((c + 2*f + k) - (a + 2*d + g)) / (8 * abs(dx))\n",
    "            q = ((c + 2*b + a) - (k + 2*h + g)) / (8 * abs(dy))\n",
    "            cell_slope = np.sqrt(p*p + q*q)\n",
    "            S[i, j] = (180 / np.pi) * np.arctan(cell_slope)\n",
    "            A[i, j] = (180.0 / np.pi) * np.arctan2(q, p)\n",
    "\n",
    "    return S, A\n",
    "\n",
    "\n",
    "def calculate_circular_mean_aspect(a):\n",
    "    \"\"\"\n",
    "    From RavenPy:\n",
    "    https://github.com/CSHS-CWRA/RavenPy/blob/1b167749cdf5984545f8f79ef7d31246418a3b54/ravenpy/utilities/analysis.py#L118\n",
    "    \"\"\"\n",
    "    angles = a[~np.isnan(a)]\n",
    "    n = len(angles)\n",
    "    sine_mean = np.divide(np.sum(np.sin(np.radians(angles))), n)\n",
    "    cosine_mean = np.divide(np.sum(np.cos(np.radians(angles))), n)\n",
    "    vector_mean = np.arctan2(sine_mean, cosine_mean)\n",
    "    degrees = np.degrees(vector_mean)\n",
    "    if degrees < 0:\n",
    "        return degrees + 360\n",
    "    else:\n",
    "        return degrees\n",
    "\n",
    "\n",
    "def calculate_slope_and_aspect(raster):  \n",
    "    \"\"\"Calculate mean basin slope and aspect \n",
    "    according to Hill (1981).\n",
    "\n",
    "    Args:\n",
    "        clipped_raster (array): dem raster\n",
    "\n",
    "    Returns:\n",
    "        slope, aspect: scalar mean values\n",
    "    \"\"\"\n",
    "\n",
    "    resolution = raster.rio.resolution()\n",
    "    raster_shape = raster[0].shape\n",
    "\n",
    "    el_px = np.argwhere(np.isfinite(raster.data[0]))\n",
    "\n",
    "    S, A = process_slope_and_aspect(raster.data[0], el_px, resolution, raster_shape)\n",
    "\n",
    "    mean_slope_deg = np.nanmean(S)\n",
    "    # should be within a hundredth of a degree or so.\n",
    "    # print(f'my slope: {mean_slope_deg:.4f}, rdem: {np.nanmean(slope):.4f}')\n",
    "    mean_aspect_deg = calculate_circular_mean_aspect(A)\n",
    "\n",
    "    return mean_slope_deg, mean_aspect_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bfaded4d-e7f6-4fe7-8bdb-0da6ed399565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_climate_data_by_basin(basin, data, param):\n",
    "    \"\"\"\n",
    "    Clip the daymet data by parameter for each basin polygon,\n",
    "    and calculate the annual spatial mean. \n",
    "    Return an array of annual spatial mean values, 1980-2022.\n",
    "    \"\"\"\n",
    "    basin = basin.to_crs(data.rio.crs)\n",
    "    clipped_data = data.rio.clip(basin.geometry, all_touched=True)        \n",
    "    spatial_means = clipped_data.mean(dim=['y', 'x'])\n",
    "    return spatial_means[param].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c9d46121-ceda-45d0-bce8-887d95ecc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2010\n",
    "nalcms_fpath = 'data/geospatial_layers/NALCMS_2010_Vancouver_Island.tif'\n",
    "nalcms, nalcms_crs, nalcms_affine = retrieve_raster(nalcms_fpath)\n",
    "if not nalcms_crs:\n",
    "    nalcms_crs = nalcms.rio.crs.to_wkt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b171aead-6125-4371-8555-110a1b05eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_data = {}\n",
    "for param in daymet_params:\n",
    "    daymet_data[param] = xr.open_dataset(f'data/daymet_data/{param}_1980_to_2022.nc', decode_coords='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1be50b8a-b4b3-4e48-939d-e8d68f05fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test, try on a few samples, note how basins_df is sliced in the for .. statement next \n",
    "# to run the whole batch, remove the [:n_samples] slice\n",
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "96d01857-1444-41f6-9dcd-6f0b16227a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ...Processing 0/1599\n",
      "POINT (1168779.4164190402 403761.5747542042)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asdfsd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m basin_polygon \u001b[38;5;241m=\u001b[39m basins_df\u001b[38;5;241m.\u001b[39miloc[[i]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(basin_polygon\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mcentroid\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43masdfsd\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfsd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "43830042-2cfa-476a-9562-eda778743a62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ...Processing 0/1599\n",
      "    ...Processing 10/1599\n",
      "Land cover category 15 is found: 0.22090556652768634%\n",
      "{1.0: 0.33889254626581816, 5.0: 0.009436032477507132, 6.0: 0.3290907760953844, 8.0: 0.04067003145344159, 10.0: 0.016823933874625118, 15.0: 0.22090556652768634, 16.0: 0.0021212786189744714, 17.0: 0.0420598346865628}\n",
      "    ...Processing 20/1599\n",
      "Land cover category 15 is found: 0.26270456503014644%\n",
      "{1.0: 0.16709732988802756, 5.0: 0.01937984496124031, 6.0: 0.019810508182601206, 8.0: 0.18906115417743324, 10.0: 0.008397932816537468, 15.0: 0.26270456503014644, 16.0: 0.017011197243755383, 17.0: 0.18087855297157623, 18.0: 0.13458225667527993, 19.0: 0.0010766580534022395}\n",
      "Land cover category 15 is found: 0.31935246504782927%\n",
      "{0.0: 0.002207505518763797, 1.0: 0.10301692420897719, 5.0: 0.005886681383370125, 6.0: 0.07579102281089035, 8.0: 0.002207505518763797, 14.0: 0.003679175864606328, 15.0: 0.31935246504782927, 17.0: 0.4878587196467991}\n",
      "    ...Processing 30/1599\n",
      "    ...Processing 40/1599\n",
      "    ...Processing 50/1599\n",
      "    ...Processing 60/1599\n",
      "    ...Processing 70/1599\n",
      "Land cover category 15 is found: 0.30714285714285716%\n",
      "{1.0: 0.4872448979591837, 5.0: 0.004591836734693878, 6.0: 0.014285714285714285, 8.0: 0.006122448979591836, 10.0: 0.019387755102040816, 14.0: 0.02295918367346939, 15.0: 0.30714285714285716, 16.0: 0.006122448979591836, 17.0: 0.12959183673469388, 18.0: 0.002551020408163265}\n",
      "    ...Processing 80/1599\n",
      "    ...Processing 90/1599\n",
      "Land cover category 15 is found: 0.46153846153846156%\n",
      "{1.0: 0.13803019410496045, 5.0: 0.012221423436376708, 6.0: 0.0873472322070453, 8.0: 0.13156002875629044, 10.0: 0.0010783608914450035, 15.0: 0.46153846153846156, 16.0: 0.0035945363048166786, 17.0: 0.16462976276060387}\n",
      "    ...Processing 100/1599\n",
      "    ...Processing 110/1599\n",
      "    ...Processing 120/1599\n",
      "Land cover category 15 is found: 0.4515370705244123%\n",
      "{1.0: 0.27974683544303797, 5.0: 0.005786618444846293, 6.0: 0.014828209764918625, 8.0: 0.0012658227848101266, 10.0: 0.0003616636528028933, 14.0: 0.11030741410488246, 15.0: 0.4515370705244123, 17.0: 0.1318264014466546, 18.0: 0.00433996383363472}\n",
      "    ...Processing 130/1599\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    ...Processing 140/1599\n",
      "    ...Processing 150/1599\n",
      "    ...Processing 160/1599\n",
      "    ...Processing 170/1599\n",
      "Land cover category 15 is found: 0.015378974458311425%\n",
      "{0.0: 2.5366429605851806e-05, 1.0: 0.6827812675835478, 5.0: 0.013974596673769267, 6.0: 0.17920690704817777, 8.0: 0.06410096761398751, 10.0: 0.01972124599902224, 15.0: 0.015378974458311425, 16.0: 0.008958961728975841, 17.0: 0.00948243259447842, 18.0: 0.00636927987012388}\n",
      "    ...Processing 180/1599\n",
      "Land cover category 15 is found: 0.0343558282208589%\n",
      "{0.0: 0.001226993865030675, 1.0: 0.2656441717791411, 5.0: 0.016564417177914112, 6.0: 0.16809815950920245, 8.0: 0.2282208588957055, 10.0: 0.015950920245398775, 15.0: 0.0343558282208589, 16.0: 0.020858895705521473, 17.0: 0.249079754601227}\n",
      "    ...Processing 190/1599\n",
      "    ...Processing 200/1599\n",
      "    ...Processing 210/1599\n",
      "    ...Processing 220/1599\n",
      "    ...Processing 230/1599\n",
      "    ...Processing 240/1599\n",
      "    ...Processing 250/1599\n",
      "    ...Processing 260/1599\n",
      "    ...Processing 270/1599\n",
      "Land cover category 15 is found: 0.48114754098360657%\n",
      "{1.0: 0.037978142076502734, 5.0: 0.012841530054644808, 6.0: 0.08415300546448087, 8.0: 0.1325136612021858, 10.0: 0.011202185792349727, 15.0: 0.48114754098360657, 16.0: 0.03715846994535519, 17.0: 0.19699453551912569, 18.0: 0.006010928961748634}\n",
      "Land cover category 15 is found: 0.024128686327077747%\n",
      "{1.0: 0.08512064343163539, 5.0: 0.1467828418230563, 6.0: 0.5636729222520107, 8.0: 0.1126005361930295, 10.0: 0.006032171581769437, 15.0: 0.024128686327077747, 17.0: 0.06166219839142091}\n",
      "    ...Processing 280/1599\n",
      "    ...Processing 290/1599\n",
      "    ...Processing 300/1599\n",
      "Land cover category 15 is found: 0.07395227253562695%\n",
      "{1.0: 0.5718020069145796, 5.0: 0.007842145206172527, 6.0: 0.2373724597352222, 8.0: 0.05599123028923181, 10.0: 0.004469179526098322, 15.0: 0.07395227253562695, 16.0: 0.0030356691120667846, 17.0: 0.04553503668100177}\n",
      "    ...Processing 310/1599\n",
      "Land cover category 15 is found: 0.020097020097020097%\n",
      "{1.0: 0.24024024024024024, 5.0: 0.031185031185031187, 6.0: 0.322014322014322, 8.0: 0.09655809655809656, 10.0: 0.01155001155001155, 15.0: 0.020097020097020097, 16.0: 0.007854007854007854, 17.0: 0.2705012705012705}\n",
      "Land cover category 15 is found: 0.03638527054108216%\n",
      "{1.0: 0.5132765531062125, 5.0: 0.025801603206412827, 6.0: 0.3193261523046092, 8.0: 0.06350200400801603, 10.0: 0.0281187374749499, 15.0: 0.03638527054108216, 16.0: 0.0013777555110220442, 17.0: 0.01221192384769539}\n",
      "    ...Processing 320/1599\n",
      "Land cover category 15 is found: 0.06861156131820638%\n",
      "{1.0: 0.31064289573203674, 5.0: 0.010804970286331712, 6.0: 0.2614802809292274, 8.0: 0.07077255537547272, 10.0: 0.007023230686115613, 15.0: 0.06861156131820638, 16.0: 0.01350621285791464, 17.0: 0.25715829281469477}\n",
      "    ...Processing 330/1599\n",
      "    ...Processing 340/1599\n",
      "    ...Processing 350/1599\n",
      "    ...Processing 360/1599\n",
      "    ...Processing 370/1599\n",
      "    ...Processing 380/1599\n",
      "    ...Processing 390/1599\n",
      "    ...Processing 400/1599\n",
      "    ...Processing 410/1599\n",
      "    ...Processing 420/1599\n",
      "Land cover category 15 is found: 0.14669106434888685%\n",
      "{1.0: 0.08539188777066178, 5.0: 0.07166819152180542, 6.0: 0.0969807868252516, 8.0: 0.4092711192436719, 10.0: 0.01738334858188472, 15.0: 0.14669106434888685, 16.0: 0.007319304666056725, 17.0: 0.16529429704178103}\n",
      "Land cover category 15 is found: 0.08033115581139642%\n",
      "{1.0: 0.40514843800530365, 5.0: 0.012871095013259169, 6.0: 0.3406636052001811, 8.0: 0.07140547183235237, 10.0: 0.010542655714378112, 15.0: 0.08033115581139642, 16.0: 0.009895867020244486, 17.0: 0.0649375848910161, 18.0: 0.004204126511868573}\n",
      "    ...Processing 430/1599\n",
      "    ...Processing 440/1599\n",
      "    ...Processing 450/1599\n",
      "    ...Processing 460/1599\n",
      "    ...Processing 470/1599\n",
      "    ...Processing 480/1599\n",
      "Land cover category 15 is found: 0.010077836952068825%\n",
      "{1.0: 0.6540761982793937, 5.0: 0.01245391233101188, 6.0: 0.18295780417861532, 8.0: 0.05399426464563704, 10.0: 0.0027857435477263415, 15.0: 0.010077836952068825, 16.0: 0.014174518639901679, 17.0: 0.06677591151167554, 18.0: 0.0027038099139696844}\n",
      "    ...Processing 490/1599\n",
      "Land cover category 15 is found: 0.013108266864527299%\n",
      "{1.0: 0.48797378346627096, 5.0: 0.013541086996846596, 6.0: 0.34687442033017996, 8.0: 0.10752488715760836, 10.0: 0.013417424101898225, 15.0: 0.013108266864527299, 16.0: 0.0016076176343288197, 17.0: 0.015581524763494713, 18.0: 0.0003709886848451122}\n",
      "    ...Processing 500/1599\n",
      "    ...Processing 510/1599\n",
      "    ...Processing 520/1599\n",
      "    ...Processing 530/1599\n",
      "    ...Processing 540/1599\n",
      "    ...Processing 550/1599\n",
      "Land cover category 15 is found: 0.14297658862876253%\n",
      "{1.0: 0.011204013377926422, 5.0: 0.0362876254180602, 6.0: 0.27775919732441473, 8.0: 0.09013377926421405, 10.0: 0.0068561872909699, 15.0: 0.14297658862876253, 16.0: 0.006187290969899666, 17.0: 0.4285953177257525}\n",
      "Land cover category 15 is found: 0.016246498599439777%\n",
      "{0.0: 0.0011204481792717086, 1.0: 0.08515406162464986, 5.0: 0.010084033613445379, 6.0: 0.012605042016806723, 8.0: 0.12549019607843137, 10.0: 0.08879551820728292, 15.0: 0.016246498599439777, 16.0: 0.03417366946778711, 17.0: 0.6263305322128851}\n",
      "    ...Processing 560/1599\n",
      "    ...Processing 570/1599\n",
      "    ...Processing 580/1599\n",
      "Land cover category 15 is found: 0.12849407288160397%\n",
      "{0.0: 0.00043904580711254205, 1.0: 0.3330894189960486, 5.0: 0.006878384311429826, 6.0: 0.22976730572223036, 8.0: 0.13493341138592127, 10.0: 0.01712278647738914, 15.0: 0.12849407288160397, 16.0: 0.018147226693985073, 17.0: 0.13112834772427923}\n",
      "    ...Processing 590/1599\n",
      "Land cover category 15 is found: 0.1830208213505119%\n",
      "{1.0: 0.12837915564247096, 5.0: 0.0784539284481767, 6.0: 0.45427355343379733, 8.0: 0.08777177038996894, 10.0: 0.005406649027953526, 15.0: 0.1830208213505119, 16.0: 0.0006902105142068331, 17.0: 0.061428735764408146, 18.0: 0.0005751754285056943}\n",
      "    ...Processing 600/1599\n",
      "    ...Processing 610/1599\n",
      "    ...Processing 620/1599\n",
      "    ...Processing 630/1599\n",
      "Land cover category 15 is found: 0.012262563116133687%\n",
      "{0.0: 0.00048088482808367395, 1.0: 0.821591728780957, 5.0: 0.004327963452753066, 6.0: 0.03125751382543881, 8.0: 0.0024044241404183697, 10.0: 0.054580427987496995, 14.0: 0.02067804760759798, 15.0: 0.012262563116133687, 17.0: 0.04784804039432556, 18.0: 0.0045684058667949026}\n",
      "    ...Processing 640/1599\n",
      "    ...Processing 650/1599\n",
      "    ...Processing 660/1599\n",
      "    ...Processing 670/1599\n",
      "Land cover category 15 is found: 0.020664869721473494%\n",
      "{1.0: 0.7389937106918238, 5.0: 0.0013477088948787063, 6.0: 0.10669362084456424, 8.0: 0.03481581311769991, 10.0: 0.0038185085354896678, 15.0: 0.020664869721473494, 16.0: 0.005615453728661276, 17.0: 0.018418688230008983, 18.0: 0.06963162623539983}\n",
      "    ...Processing 680/1599\n",
      "    ...Processing 690/1599\n",
      "    ...Processing 700/1599\n",
      "Land cover category 15 is found: 0.016594516594516596%\n",
      "{1.0: 0.1378066378066378, 5.0: 0.007215007215007215, 6.0: 0.0873015873015873, 8.0: 0.45815295815295815, 10.0: 0.07575757575757576, 15.0: 0.016594516594516596, 16.0: 0.08008658008658008, 17.0: 0.13275613275613277, 18.0: 0.004329004329004329}\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    ...Processing 710/1599\n",
      "    ...Processing 720/1599\n",
      "    ...Processing 730/1599\n",
      "    ...Processing 740/1599\n",
      "Land cover category 15 is found: 0.022533831363664197%\n",
      "{1.0: 0.4297960933194538, 5.0: 0.0372910415773682, 6.0: 0.3729716490110832, 8.0: 0.11499601983956892, 10.0: 0.009491151797195518, 15.0: 0.022533831363664197, 16.0: 0.004041393668483253, 17.0: 0.004959892229502174, 18.0: 0.00391892719368073}\n",
      "    ...Processing 750/1599\n",
      "Land cover category 15 is found: 0.018868275099636754%\n",
      "{1.0: 0.5097685826032832, 5.0: 0.025064798729108797, 6.0: 0.10149478358617998, 8.0: 0.12308506981540492, 10.0: 0.010925205079848756, 15.0: 0.018868275099636754, 16.0: 0.010451407920773682, 17.0: 0.17271299969342538, 18.0: 0.027628877472338605}\n",
      "    ...Processing 760/1599\n",
      "    ...Processing 770/1599\n",
      "    ...Processing 780/1599\n",
      "Land cover category 15 is found: 0.02820184790334044%\n",
      "{1.0: 0.543681592039801, 5.0: 0.02914001421464108, 6.0: 0.2452594171997157, 8.0: 0.10803127221037669, 10.0: 0.008386638237384506, 15.0: 0.02820184790334044, 16.0: 0.005771144278606965, 17.0: 0.02962331201137171, 18.0: 0.0019047619047619048}\n",
      "Land cover category 15 is found: 0.1697530864197531%\n",
      "{1.0: 0.26157407407407407, 5.0: 0.005401234567901234, 6.0: 0.009259259259259259, 8.0: 0.029320987654320986, 10.0: 0.1597222222222222, 14.0: 0.2623456790123457, 15.0: 0.1697530864197531, 17.0: 0.10262345679012345}\n",
      "    ...Processing 790/1599\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    ...Processing 800/1599\n",
      "Land cover category 15 is found: 0.012621140410187063%\n",
      "{1.0: 0.0676132521974307, 5.0: 0.0820374126662159, 6.0: 0.5566824430921794, 8.0: 0.16339869281045752, 10.0: 0.004732927653820149, 15.0: 0.012621140410187063, 16.0: 0.007662835249042145, 17.0: 0.10525129592066712}\n",
      "    ...Processing 810/1599\n",
      "    ...Processing 820/1599\n",
      "    ...Processing 830/1599\n",
      "    ...Processing 840/1599\n",
      "Land cover category 15 is found: 0.12467932272960493%\n",
      "{1.0: 0.11287839917906618, 5.0: 0.0107747562852745, 6.0: 0.15084658799384298, 8.0: 0.3478707029245767, 10.0: 0.05489994869163674, 15.0: 0.12467932272960493, 16.0: 0.049769112365315546, 17.0: 0.14417650076962546, 18.0: 0.004104669061056952}\n",
      "    ...Processing 850/1599\n",
      "    ...Processing 860/1599\n",
      "    ...Processing 870/1599\n",
      "    ...Processing 880/1599\n",
      "    ...Processing 890/1599\n",
      "    ...Processing 900/1599\n",
      "    ...Processing 910/1599\n",
      "    ...Processing 920/1599\n",
      "Land cover category 15 is found: 0.015966202235326545%\n",
      "{1.0: 0.4672921414179843, 5.0: 0.04596336828538264, 6.0: 0.23765581872037456, 8.0: 0.09450155479034593, 10.0: 0.011016495141522802, 15.0: 0.015966202235326545, 16.0: 0.007986497975472746, 17.0: 0.042216148856132396, 18.0: 0.07740177257745806}\n",
      "Land cover category 15 is found: 0.0193621867881549%\n",
      "{1.0: 0.3682611996962794, 5.0: 0.008731966590736523, 6.0: 0.5474563401670464, 8.0: 0.011389521640091117, 15.0: 0.0193621867881549, 17.0: 0.044798785117691725}\n",
      "    ...Processing 930/1599\n",
      "    ...Processing 940/1599\n",
      "Land cover category 15 is found: 0.3368660105980318%\n",
      "{1.0: 0.13701741105223317, 5.0: 0.03785011355034065, 6.0: 0.15518546555639667, 8.0: 0.050719152157456475, 10.0: 0.25510976532929597, 15.0: 0.3368660105980318, 16.0: 0.000757002271006813, 17.0: 0.026495079485238455}\n",
      "    ...Processing 950/1599\n",
      "    ...Processing 960/1599\n",
      "Land cover category 15 is found: 0.03980748019653063%\n",
      "{0.0: 0.0002005414619472576, 1.0: 0.7665697382933921, 5.0: 0.002807580467261606, 6.0: 0.08763661887095157, 8.0: 0.026270931515090745, 10.0: 0.004211370700892409, 15.0: 0.03980748019653063, 16.0: 0.0025067682743407198, 17.0: 0.038905043617767976, 18.0: 0.031083926601824928}\n",
      "    ...Processing 970/1599\n",
      "    ...Processing 980/1599\n",
      "    ...Processing 990/1599\n",
      "    ...Processing 1000/1599\n",
      "    ...Processing 1010/1599\n",
      "    ...Processing 1020/1599\n",
      "    ...Processing 1030/1599\n",
      "Land cover category 15 is found: 0.3028443948689347%\n",
      "{1.0: 0.09871723368655884, 5.0: 0.04182933630786392, 6.0: 0.26659230340211937, 8.0: 0.07417735638594534, 10.0: 0.009481316229782488, 15.0: 0.3028443948689347, 16.0: 0.006134969325153374, 17.0: 0.20022308979364195}\n",
      "    ...Processing 1040/1599\n",
      "    ...Processing 1050/1599\n",
      "    ...Processing 1060/1599\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    ...Processing 1070/1599\n",
      "    ...Processing 1080/1599\n",
      "    ...Processing 1090/1599\n",
      "Land cover category 15 is found: 0.2518012192869019%\n",
      "{1.0: 0.11897284315536671, 5.0: 0.036209126177720305, 6.0: 0.3345649362645483, 8.0: 0.1852946610012932, 10.0: 0.004803251431738408, 15.0: 0.2518012192869019, 16.0: 0.03528542397930907, 17.0: 0.03306853870312211}\n",
      "    ...Processing 1100/1599\n",
      "Land cover category 15 is found: 0.07604294061693165%\n",
      "{1.0: 0.14442179643973366, 5.0: 0.08441364315803777, 6.0: 0.2511482538388368, 8.0: 0.3511890202473162, 10.0: 0.04549531186302487, 15.0: 0.07604294061693165, 16.0: 0.01114281831770621, 17.0: 0.0321782850930833, 18.0: 0.003967930425329528}\n",
      "    ...Processing 1110/1599\n",
      "    ...Processing 1120/1599\n",
      "Land cover category 15 is found: 0.014303568948757117%\n",
      "{0.0: 0.00041660880433273156, 1.0: 0.38744618802944036, 5.0: 0.05068740452714901, 6.0: 0.41660880433273156, 8.0: 0.055964449382030275, 10.0: 0.004582696847660047, 15.0: 0.014303568948757117, 16.0: 0.004582696847660047, 17.0: 0.06346340786001944, 18.0: 0.001944174420219414}\n",
      "    ...Processing 1130/1599\n",
      "    ...Processing 1140/1599\n",
      "    ...Processing 1150/1599\n",
      "    ...Processing 1160/1599\n",
      "    ...Processing 1170/1599\n",
      "    ...Processing 1180/1599\n",
      "    ...Processing 1190/1599\n",
      "    ...Processing 1200/1599\n",
      "    ...Processing 1210/1599\n",
      "Land cover category 15 is found: 0.033816425120772944%\n",
      "{1.0: 0.5226174791392183, 5.0: 0.04040404040404041, 6.0: 0.19938515590689504, 8.0: 0.05357927097057532, 10.0: 0.010540184453227932, 15.0: 0.033816425120772944, 16.0: 0.06104523495827844, 17.0: 0.07861220904699165}\n",
      "    ...Processing 1220/1599\n",
      "    ...Processing 1230/1599\n",
      "    ...Processing 1240/1599\n",
      "Land cover category 15 is found: 0.08707286949361877%\n",
      "{1.0: 0.3207081103334706, 5.0: 0.014203375874845615, 6.0: 0.18423219431864965, 8.0: 0.04960889254837381, 10.0: 0.005351996706463565, 15.0: 0.08707286949361877, 16.0: 0.01214491560312886, 17.0: 0.32482503087690406, 18.0: 0.0018526142445450802}\n",
      "    ...Processing 1250/1599\n",
      "    ...Processing 1260/1599\n",
      "Land cover category 15 is found: 0.039491298527443104%\n",
      "{1.0: 0.8440428380187416, 5.0: 0.009370816599732263, 6.0: 0.049531459170013385, 10.0: 0.01606425702811245, 14.0: 0.020080321285140562, 15.0: 0.039491298527443104, 16.0: 0.0013386880856760374, 17.0: 0.020080321285140562}\n",
      "    ...Processing 1270/1599\n",
      "Land cover category 15 is found: 0.016777801677780167%\n",
      "{1.0: 0.40675414067541404, 5.0: 0.06079508789769061, 6.0: 0.3088445217935431, 8.0: 0.13559123174094134, 10.0: 0.023269911417900233, 15.0: 0.016777801677780167, 16.0: 0.016308492539940163, 17.0: 0.031111284929310312, 18.0: 0.0005475273274800055}\n",
      "    ...Processing 1280/1599\n",
      "Land cover category 15 is found: 0.1544055944055944%\n",
      "{1.0: 0.4646153846153846, 5.0: 0.017342657342657344, 6.0: 0.08895104895104895, 8.0: 0.0033566433566433566, 10.0: 0.03776223776223776, 14.0: 0.1283916083916084, 15.0: 0.1544055944055944, 17.0: 0.09734265734265735, 18.0: 0.007832167832167832}\n",
      "    ...Processing 1290/1599\n",
      "    ...Processing 1300/1599\n",
      "    ...Processing 1310/1599\n",
      "Land cover category 15 is found: 0.14348206474190725%\n",
      "{1.0: 0.3416447944006999, 5.0: 0.027668416447944006, 6.0: 0.2975721784776903, 8.0: 0.0770997375328084, 10.0: 0.003390201224846894, 15.0: 0.14348206474190725, 16.0: 0.005905511811023622, 17.0: 0.10236220472440945, 18.0: 0.0008748906386701663}\n",
      "    ...Processing 1320/1599\n",
      "    ...Processing 1330/1599\n",
      "    ...Processing 1340/1599\n",
      "    ...Processing 1350/1599\n",
      "    ...Processing 1360/1599\n",
      "    ...Processing 1370/1599\n",
      "    ...Processing 1380/1599\n",
      "    ...Processing 1390/1599\n",
      "    ...Processing 1400/1599\n",
      "Land cover category 15 is found: 0.24435590969455512%\n",
      "{1.0: 0.1254980079681275, 5.0: 0.05245683930942895, 6.0: 0.4196547144754316, 8.0: 0.12881806108897742, 15.0: 0.24435590969455512, 16.0: 0.0026560424966799467, 17.0: 0.025896414342629483, 18.0: 0.0006640106241699867}\n",
      "    ...Processing 1410/1599\n",
      "    ...Processing 1420/1599\n",
      "Land cover category 15 is found: 0.05940482027684532%\n",
      "{1.0: 0.5878814166635236, 5.0: 0.018531789436628573, 6.0: 0.13082890154515395, 8.0: 0.12200304253259407, 10.0: 0.016897371100969335, 15.0: 0.05940482027684532, 16.0: 0.012893046178604206, 17.0: 0.03742817988659651, 18.0: 0.014049711462301512, 19.0: 8.172091678296181e-05}\n",
      "    ...Processing 1430/1599\n",
      "    ...Processing 1440/1599\n",
      "Land cover category 15 is found: 0.10110660526496666%\n",
      "{0.0: 6.873324627122139e-05, 1.0: 0.15306893944601002, 5.0: 0.05010653653172039, 6.0: 0.4385868444566637, 8.0: 0.1472266135129562, 10.0: 0.008041789813732903, 15.0: 0.10110660526496666, 16.0: 0.00989758746305588, 17.0: 0.09189635026462299}\n",
      "    ...Processing 1450/1599\n",
      "    ...Processing 1460/1599\n",
      "    ...Processing 1470/1599\n",
      "    ...Processing 1480/1599\n",
      "    ...Processing 1490/1599\n",
      "Land cover category 15 is found: 0.1386861313868613%\n",
      "{1.0: 0.058394160583941604, 5.0: 0.032981886996485535, 6.0: 0.23952419572857528, 8.0: 0.04082184374155177, 15.0: 0.1386861313868613, 16.0: 0.0021627466882941336, 17.0: 0.48742903487429035}\n",
      "    ...Processing 1500/1599\n",
      "Land cover category 15 is found: 0.21529745042492918%\n",
      "{1.0: 0.09728302858614474, 5.0: 0.03811485964460469, 6.0: 0.248004120525367, 8.0: 0.2232165851146021, 10.0: 0.01667525109451455, 15.0: 0.21529745042492918, 16.0: 0.007854751480813804, 17.0: 0.15310327066701004, 18.0: 0.0004506824620139068}\n",
      "    ...Processing 1510/1599\n",
      "    ...Processing 1520/1599\n",
      "Land cover category 15 is found: 0.13285682431930232%\n",
      "{0.0: 2.4775165374228874e-05, 1.0: 0.12583306493570845, 5.0: 0.0924980799246835, 6.0: 0.3069395238213215, 8.0: 0.25606372172534253, 10.0: 0.02465128954735773, 15.0: 0.13285682431930232, 16.0: 0.011347025741396824, 17.0: 0.04634194683249511, 18.0: 0.0034437479870178134}\n",
      "    ...Processing 1530/1599\n",
      "    ...Processing 1540/1599\n",
      "    ...Processing 1550/1599\n",
      "    ...Processing 1560/1599\n",
      "    ...Processing 1570/1599\n",
      "    ...Processing 1580/1599\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    Area proportions do not sum to 1: 0.00\n",
      "    ...Processing 1590/1599\n",
      "Land cover category 15 is found: 0.19067848039810656%\n",
      "{1.0: 0.5662094914431363, 5.0: 0.004976332079135818, 6.0: 0.04248088360237893, 8.0: 0.0054618278917344335, 10.0: 0.028401505037019055, 14.0: 0.03422745478820245, 15.0: 0.19067848039810656, 16.0: 0.005825949751183396, 17.0: 0.12113120524335477, 18.0: 0.0006068697657482705}\n"
     ]
    }
   ],
   "source": [
    "all_basin_data = []\n",
    "t0 = time.time()\n",
    "# for i, row in basins_df[:n_samples].iterrows():\n",
    "for i, row in basins_df.iterrows():\n",
    "    if i % 10 == 0:\n",
    "        print(f'    ...Processing {i}/{len(basins_df)}')\n",
    "    basin_data = {}\n",
    "    basin_data['region'] = region\n",
    "    basin_data['id'] = row['VALUE']\n",
    "\n",
    "    basin_polygon = basins_df.iloc[[i]].copy()\n",
    "    \n",
    "    clip_ok, clipped_dem = clip_raster_to_basin(basin_polygon, region_dem)    \n",
    "    \n",
    "    land_cover = process_lulc(basin_polygon, nalcms)\n",
    "    land_cover = land_cover.to_dict('records')[0]\n",
    "    # print('     lulc complate')\n",
    "    basin_data.update(land_cover)\n",
    "    \n",
    "    soil = process_glhymps(basin_polygon, glhymps_path)\n",
    "    porosity = get_soil_properties(soil, 'Porosity')\n",
    "    permeability = get_soil_properties(soil, 'Permeability_no_permafrost')\n",
    "    basin_data['Permeability_logk_m2'] = round(permeability, 2)\n",
    "    basin_data['Porosity_frac'] = round(porosity, 5)\n",
    "    \n",
    "    slope, aspect = calculate_slope_and_aspect(clipped_dem)\n",
    "    # print(f'aspect, slope: {aspect:.1f} {slope:.2f} ')\n",
    "    basin_data['Slope_deg'] = slope\n",
    "    basin_data['Aspect_deg'] = aspect\n",
    "\n",
    "\n",
    "    mean_el, median_el, min_el, max_el = process_basin_elevation(clipped_dem)\n",
    "    basin_data['median_el'] = median_el\n",
    "    basin_data['mean_el'] = mean_el\n",
    "    basin_data['max_el'] = max_el\n",
    "    basin_data['min_el'] = min_el\n",
    "    \n",
    "    # geojson only supports one geometry column\n",
    "    basin_data['geometry'] = basin_polygon.geometry.values[0]\n",
    "    basin_data['pour_pt'] = basin_polygon.geometry.values[0]\n",
    "    basin_data['basin_centroid'] = basin_polygon.geometry.centroid.values[0]\n",
    "\n",
    "    for climate_param in daymet_params:\n",
    "        test_basin = basins_df.loc[[0]]\n",
    "        basin_data[climate_param] = process_climate_data_by_basin(basin_polygon, daymet_data[climate_param], climate_param)\n",
    "    \n",
    "    all_basin_data.append(basin_data)\n",
    "\n",
    "t1 = time.time()\n",
    "ut = (t1 - t0) / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a43406fd-d1a7-4aeb-bdf1-f60608eb4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 basins in 2219s (221.93s/basin)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1332628/4194047413.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Processed {n_samples} basins in {t1-t0:.0f}s ({ut:.2f}s/basin)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# save the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.geojson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'notebooks/data/{output_fname}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'notebooks/data/basin_attributes.geojson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/code/23/bcub_env/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_file'"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame(all_basin_data)\n",
    "print(f'Processed {n_samples} basins in {t1-t0:.0f}s ({ut:.2f}s/basin)')\n",
    "# save the file\n",
    "output_fname = file.replace('.geojson', '.csv')\n",
    "output.to_csv(os.path.join(base_dir, f'notebooks/data/{output_fname}'))\n",
    "output.to_file(os.path.join(base_dir, f'notebooks/data/basin_attributes.geojson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cbc40-e11c-4185-a2a3-cf6cb8bdb4de",
   "metadata": {},
   "source": [
    "Median processing time is roughly 1s/basin on a ~2018 Intel i7-8850H CPU @ 2.60GHz.  Processing time is proporti onal to the size of clipped DEM, and using JIT in the `process_slope_and_aspect` function yields ~3X performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02febb16-be82-4dfd-8fc5-f09e69d43a34",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography} \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
