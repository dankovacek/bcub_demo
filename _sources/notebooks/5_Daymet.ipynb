{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4abe2d8-1b65-4f6c-95e7-86b0aac222d6",
   "metadata": {},
   "source": [
    "# Climate Attribute Development- NASA Daymet\n",
    "\n",
    "```{figure} img/animated_years.gif\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "NASA Daymet data is spatially distributed time series climate data covering North America and spanning 1980 to 2022.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e779c4-976e-452d-90f6-74e16e573003",
   "metadata": {},
   "source": [
    "## Accessing and Registering for NASA DAYMET Data\n",
    "\n",
    "NASA's DAYMET provides daily surface weather and climatological summaries for North America.  To access and automate the download of DAYMET data, follow these steps:\n",
    "\n",
    "1. **Register**: Before you can download data, you need to [register with ORNL DAAC](https://urs.earthdata.nasa.gov/). \n",
    "   \n",
    "2. **Access the Data**: Once registered, navigate to the [DAYMET Data Collection page](https://daymet.ornl.gov/) where you can explore available data sets.\n",
    "   \n",
    "3. **Automated Download**: For automated data downloads, you can use the DAYMET web services. Detailed instructions and examples for using these services can be found in the [DAYMET documentation](https://daymet.ornl.gov/web_services.html).\n",
    "\n",
    "Available climate variables are described in the [Daymet Catalogue](https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/2129/catalog.html):\n",
    "\n",
    "| Variable | Description | Units |\n",
    "|---|---|---|\n",
    "| tmax | Daily maximum 2-meter air temperature | °C |\n",
    "| tmin | Daily minimum 2-meter air temperature | °C |\n",
    "| prcp | Daily total precipitation | mm |\n",
    "| srad | Incident shortwave radiation flux density | $W/m^2$ |\n",
    "| vp | Water vapor pressure | Pa |\n",
    "| swe | Snow water equivalent | $kg/m^2$ |\n",
    "| dayl | Duration of the daylight period | seconds/day |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07118ff9-ac47-4adf-8e62-aaf46faf342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62179766-525d-4d4b-a909-3b2d96b9af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'Vancouver_Island'\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "polygon_path = os.path.join(os.getcwd(), f'data/region_polygons/{region}.geojson')\n",
    "region_polygon = gpd.read_file(polygon_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d4140-2fe1-4c25-9118-d44934ed6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_proj = '+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs'\n",
    "daymet_folder ='/media/danbot/Samsung_T51/large_sample_hydrology/common_data/DAYMET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1ee3e-087b-4a6a-b8c2-dee59235910d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the daymet tile index\n",
    "tile_fpath = os.path.join(base_dir, 'notebooks/data/daymet_data/Daymet_v4_Tiles.geojson')\n",
    "dm_tiles = gpd.read_file(tile_fpath)\n",
    "print(f'daymet tile index is in {dm_tiles.crs}')\n",
    "\n",
    "# get the intersection with the region polygon\n",
    "tiles_df = dm_tiles.sjoin(region_polygon)\n",
    "tiles_df = tiles_df.sort_values(by=['Latitude (Min)', 'Longitude (Min)'])\n",
    "tile_rows = tiles_df.groupby('Latitude (Min)')['TileID'].apply(list).tolist()\n",
    "tile_ids = tiles_df['TileID'].values\n",
    "print(f'There are {len(tile_ids)} covering tiles.')\n",
    "# save the dataframe of relevant tiles to a .shp file \n",
    "# gdal is used to clip the spatial datasets with a .shp mask.\n",
    "# ensure the mask is in the same crs as the tiles.\n",
    "reproj_mask_path = polygon_path.replace('.geojson', '_daymet_mask.shp')\n",
    "if not os.path.exists(reproj_mask_path):\n",
    "    mask = region_polygon.to_crs(daymet_proj)\n",
    "    mask.to_file(reproj_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac4ab2-1ea8-4b21-b39b-02362fbdf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * 2129: Daily\n",
    "# * 2131: Monthly average\n",
    "# * 2130: Annual average\n",
    "code = 2129\n",
    "\n",
    "daymet_params = ['prcp']#'tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp']\n",
    "years = list(range(1980,2023))\n",
    "\n",
    "daymet_url_base = f'https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/{code}/tiles/'                \n",
    "base_command = f'wget -q --show-progress --progress=bar:force --limit-rate=3m {daymet_url_base}'\n",
    "\n",
    "\n",
    "code_dict = {\n",
    "    2129: 'daily', 2130: 'annual', 2131: 'monthly',\n",
    "}\n",
    "\n",
    "batch_commands = []\n",
    "for yr in years:\n",
    "    for param in daymet_params:\n",
    "        for tile in tile_ids:\n",
    "            param_dir = os.path.join(daymet_folder, param)\n",
    "            if not os.path.exists(param_dir):\n",
    "                os.mkdir(param_dir)            \n",
    "\n",
    "            file_path = f'{yr}/{tile}_{yr}/{param}.nc'\n",
    "            save_fpath = os.path.join(daymet_folder, f'{param}/{tile}_{yr}_{param}.nc')\n",
    "\n",
    "            if not os.path.exists(save_fpath):\n",
    "                cmd = base_command + f'{file_path} -O {save_fpath}'\n",
    "                batch_commands.append(cmd)\n",
    "                \n",
    "\n",
    "# # # download the files in parallel\n",
    "print(f'{len(batch_commands)} files to download')\n",
    "with mp.Pool() as pl:\n",
    "    pl.map(os.system, batch_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4152430-17ca-4673-8c9f-3013b18590ea",
   "metadata": {},
   "source": [
    "## Process NASA DAYMET Data\n",
    "\n",
    "To process the NASA DAYMET data spanning from 1980 to 2022 for specific parameters ('tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp') within a set of polygons, we will use the following approach:\n",
    "\n",
    "1. **Data Preparation**: Merge the .nc (NetCDF) tiles in an xarray dataset.\n",
    "\n",
    "2. **Temporal Statistics**: For each year and parameter:  \n",
    "  a. Load the relevant .nc files using xarray,  \n",
    "  b. Compute mean/max/total annual values for each year (use resample method on the time coordinate),  \n",
    "  c. Compute mean annual (mean of years),  \n",
    "  d. Output file as raster (tif).  \n",
    "3. **Raster Crop**: Use gdalwarp to crop the output raster to the region.\n",
    "4. **Attribute Extraction**: for the set of basins generated in Notebook 4, mask the output rasters to get 6 climate indices per basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ff87f-26ac-43d3-822a-2b259a49eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_dataset(f, grp=None):\n",
    "    \"\"\"Open a dataset using ``xarray``.\n",
    "    From pydaymet: https://github.com/hyriver/pydaymet/blob/44ca8be043f2ac27bb815ac3e70e33094da22730/pydaymet/pydaymet.py\n",
    "    \"\"\"\n",
    "    with xr.open_dataset(f, engine=\"netcdf4\", chunks={},\n",
    "                         mask_and_scale=True, cache=False) as ds:\n",
    "        return ds.load()\n",
    "\n",
    "\n",
    "def resample_annual(param, tile_id, output_path):\n",
    "    \"\"\"\n",
    "    Adapted from pydaymet get_bygeom()\n",
    "    \"\"\"\n",
    "    param_folder = os.path.join(daymet_folder, param)\n",
    "    clm_files = sorted([os.path.join(param_folder, e) for e in os.listdir(param_folder) if (e.startswith(tid) & ~e.endswith('.xml'))])   \n",
    "\n",
    "    ds = xr.concat((_open_dataset(f) for f in clm_files), dim='time')[param]    \n",
    "    #  write crs BEFORE AND AFTER resampling!\n",
    "    ds.rio.write_nodata(np.nan, inplace=True)\n",
    "    ds = ds.rio.write_crs(daymet_proj)\n",
    "    \n",
    "    if param in ['prcp']:\n",
    "        # note that sum handles nan values differently than max and mean\n",
    "        # explicitly set skipna=False or the assembled mosaic will be wonky\n",
    "        ds = ds.resample(time='1y').sum(keep_attrs=True, skipna=False)\n",
    "    elif param == 'swe':\n",
    "        # HYSETS uses average annual maximum\n",
    "        ds = ds.resample(time='1y').max(keep_attrs=True)\n",
    "    else:\n",
    "        ds = ds.resample(time='1y').mean(keep_attrs=True)\n",
    "\n",
    "    annual_mean = ds.mean('time', keep_attrs=True)\n",
    "    annual_mean.rio.write_crs(daymet_proj)\n",
    "    annual_mean.rio.write_nodata(np.nan, inplace=True)\n",
    "    annual_mean.rio.to_raster(output_fpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd906a-69cd-419e-a233-233ae08c9fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tile_mosaic(param, reproj_fpath):\n",
    "    year = 1980\n",
    "    param_folder = os.path.join(daymet_folder, param)\n",
    "    data_folder = f'data/daymet_data'\n",
    "    \n",
    "    # fpath = f'{tid}_{year}_{param}.nc'\n",
    "    file_pattern = f'*_{param}_mean_annual.tiff'\n",
    "    vrt_fname = f'{param}_mosaic.vrt'\n",
    "    vrt_fpath = os.path.join(data_folder, vrt_fname)\n",
    "\n",
    "    # warp and save the file path\n",
    "    reproj_fpath = os.path.join(data_folder, f'{param}_mosaic_3005.tiff')\n",
    "    # assemble the mosaic\n",
    "    cmd = f'gdalbuildvrt {vrt_fpath} {data_folder}/{param}/{file_pattern}'\n",
    "    # print(cmd)\n",
    "    # print(asdf)\n",
    "    warp_cmd = f'gdalwarp -multi -cutline {reproj_mask_path} -crop_to_cutline -wo CUTLINE_ALL_TOUCHED=TRUE -t_srs EPSG:3005 -co TILED=YES -co BIGTIFF=YES -co COMPRESS=DEFLATE -co NUM_THREADS=ALL_CPUS {vrt_fpath} {reproj_fpath}'\n",
    "    if not os.path.exists(vrt_fpath):\n",
    "        os.system(cmd)\n",
    "        os.system(warp_cmd)\n",
    "        os.remove(vrt_fpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387ece4-33d6-4b6e-89ac-f0f6378168ce",
   "metadata": {},
   "source": [
    "## Create raster (tif) files \n",
    "\n",
    "Given netcdf file inputs describing the 6 parameters on a daily frequency, summarize down to one spatial layer (mean annual) for each parameter that we will clip to the region polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa177fba-ecdd-4c0b-8a04-4a63f0e33861",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in daymet_params:\n",
    "    print(f'processing {param}')\n",
    "    param_folder = os.path.join(daymet_folder, param)\n",
    "    for tid in tile_ids:\n",
    "        output_file = f'{tid}_{param}_mean_annual.tiff'\n",
    "        folder = f'data/daymet_data/{param}/'\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        output_fpath = os.path.join(folder, output_file)\n",
    "        if not os.path.exists(output_fpath):\n",
    "            try:\n",
    "                resample_annual(param, tid, output_fpath)\n",
    "            except Exception as ex:\n",
    "                print(f'Resampling failed on {param} tile id: {tid}')\n",
    "                print(ex)\n",
    "                continue\n",
    "    reproj_fpath = os.path.join('data/daymet_data/', f'{param}_mosaic_3005.tiff')\n",
    "    if not os.path.exists(reproj_fpath):\n",
    "        create_tile_mosaic(param, reproj_fpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf2dfd-8151-4023-8072-013ee9a00abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
