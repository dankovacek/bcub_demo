{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7fa95f8-61cb-4cd9-aacf-6a8acaede5e6",
   "metadata": {},
   "source": [
    "# 4. Basin Delineation\n",
    "\n",
    "```{figure} img/basin_batch.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "Pour points and derived basins for batch 1/12 for Vancouver Island.\n",
    "```\n",
    "\n",
    "Large sample basin delineation is a computationally intensive operation, but the Whitebox `unnest_basins` function performs really well.  Whitebox is written in [Rust](https://www.rust-lang.org/).  Many functions implicitly take care of paralellization to run efficiently.  We'll carry out the operation by setting up batches to manage disk usage and RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec9072-763f-4bf8-8a63-2569d59feec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whitebox \n",
    "\n",
    "wbt = whitebox.WhiteboxTools()\n",
    "wbt.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce823d-aa27-40a1-a368-5259ac78ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the stream layer\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b59535-fe8c-4e52-8355-0103bf060276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.validation import make_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc23d1-dd63-408b-b481-b658969f4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_vector_basins_batch(input):\n",
    "    \"\"\"\n",
    "    If we send too many pour points in a large raster, we can \n",
    "    generate a huge number of temporary raster files that could easily \n",
    "    exceed common SSD disk capacities.\n",
    "\n",
    "    Here we set up batches to limit the temporary disk space used.\n",
    "    \"\"\"\n",
    "\n",
    "    raster_fname, raster_crs, resolution, min_area, temp_folder = input\n",
    "    raster_path = os.path.join(temp_folder, raster_fname)\n",
    "    raster_no = int(raster_fname.split('_')[-1].split('.')[0])\n",
    "    polygon_path = os.path.join(temp_folder, f'temp_polygons_{raster_no:05}.shp')\n",
    "    \n",
    "    # this function creates rasters of ordered \n",
    "    # sets of non-overlapping basins\n",
    "    wbt.raster_to_vector_polygons(\n",
    "        raster_path,\n",
    "        polygon_path,\n",
    "    )\n",
    "\n",
    "    gdf = gpd.read_file(polygon_path, crs=raster_crs)\n",
    "\n",
    "    # simplify the polygon geometry to avoid self-intersecting polygons\n",
    "    simplify_dim = 0.5 * np.sqrt(resolution[0]**2 + resolution[1]**2)\n",
    "    simplify_dim = abs(resolution[0])\n",
    "    buffer_dim = 10\n",
    "    gdf.geometry = gdf.geometry.buffer(buffer_dim)\n",
    "    gdf.geometry = gdf.geometry.simplify(simplify_dim)\n",
    "    gdf.geometry = gdf.geometry.buffer(-1.0 * buffer_dim)\n",
    "    \n",
    "    gdf = filter_and_explode_geoms(gdf, min_area)\n",
    "\n",
    "    assert (gdf.geometry.geom_type == 'Polygon').all()\n",
    "        \n",
    "    gdf.drop(labels=['area'], inplace=True, axis=1)\n",
    "    gdf.to_file(polygon_path.replace('.shp', '.geojson'))\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8d96e-581b-4650-ac82-4f5e0f915a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'Vancouver_Island'\n",
    "\n",
    "fdir_path = os.path.join(base_dir, f'data/DEM/{region}_d8_pointer.tif')\n",
    "\n",
    "# get the file size in MB\n",
    "filesize = (os.path.getsize(fdir_path) >> 20 )\n",
    "\n",
    "temp_dir = os.path.join(base_dir, f'data/temp/')        \n",
    "ppt_dir = os.path.join(base_dir, f'data/pour_points/')\n",
    "basin_output_dir = os.path.join(base_dir, f'data/basins/')\n",
    "\n",
    "for d in [temp_dir, basin_output_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c780fb-0802-489a-8f67-a19daeae8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import (retrieve_raster, redistribute_vertices, match_ppt_to_polygons_by_order,\n",
    "                        check_for_ppt_batches, create_ppt_file_batches, clean_up_temp_files,\n",
    "                        filter_and_explode_geoms)\n",
    "\n",
    "region_raster, region_raster_crs, _ = retrieve_raster(fdir_path)\n",
    "raster_resolution = tuple(abs(e) for e in region_raster.rio.resolution())\n",
    "print(f'    {region} raster crs = {region_raster_crs}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e5fee-716e-484c-b9a6-1761ea9e6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Break the pour points into unique sets\n",
    "# to avoid duplicate delineations\n",
    "ppt_file = os.path.join(ppt_dir, f'{region}_pour_points.geojson')\n",
    "\n",
    "ppt_gdf = gpd.read_file(ppt_file)\n",
    "ppt_crs = ppt_gdf.crs\n",
    "print(f'    ppt crs = {ppt_crs}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a5425-41b6-49b8-b8cc-e78dd139b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all pour point cell indices are unique\n",
    "ppt_gdf.drop_duplicates(\n",
    "    subset=['cell_idx'], \n",
    "    keep='first', inplace=True, ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de3d98-5454-4116-91be-efa55d39b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for an output file to allow for partial updating\n",
    "# by tracking (unique) ppt indices\n",
    "output_fpath = os.path.join(base_dir, f'data/basins/{region}_basins.geojson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe5306-c4bc-46e7-a551-81a3ce3fc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# check if the ppt batches have already been created\n",
    "batch_dir = os.path.join(temp_dir, 'ppt_batches/')\n",
    "batches_exist = check_for_ppt_batches(batch_dir)\n",
    "\n",
    "# break the pour point dataframe into batches to limit the RAM used in \n",
    "# processing basins for each pour point.\n",
    "temp_ppt_filepath = os.path.join(batch_dir, f'pts.shp')\n",
    "\n",
    "if not batches_exist:\n",
    "    print(f'    Generating new batch paths.')\n",
    "    batch_ppt_paths = create_ppt_file_batches(ppt_gdf, filesize, temp_ppt_filepath)\n",
    "else:\n",
    "    print(f'    Retrieving existing batch paths.')\n",
    "    batch_ppt_files = list(sorted([f for f in os.listdir(batch_dir) if f.endswith('.shp')]))\n",
    "    batch_ppt_paths = [os.path.join(batch_dir, f) for f in batch_ppt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945c9ba-63c5-4f2d-8a76-b8846c391bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_files = len(batch_ppt_paths)\n",
    "batch_output_files = sorted([f for f in os.listdir(basin_output_dir) if f.startswith(output_fpath.split('/')[-1].split('.')[0])])\n",
    "if len(batch_output_files) > 0:\n",
    "    batch_matches = [b.replace('.geojson', '.shp').split('_')[-1] for b in batch_output_files]\n",
    "    batch_ppt_paths = list(sorted([f for f in batch_ppt_paths if f.split('_')[-1] not in batch_matches]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d1c3-05b9-4726-a4f8-92dfe8e97b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_output_files = []\n",
    "min_basin_area = 1 # km^2\n",
    "for ppt_batch_path in batch_ppt_paths:\n",
    "    t_batch_start = time.time()\n",
    "    batch_no = int(ppt_batch_path.split('_')[-1].split('.')[0])\n",
    "    print(f'  Starting processing {region} batch {batch_no}/{n_batch_files}')\n",
    "    temp_fname = f'temp_raster.tif'\n",
    "    temp_basin_raster_path = os.path.join(temp_dir, temp_fname)            \n",
    "    batch_output_fpath = output_fpath.replace('.geojson', f'_{batch_no:04d}.geojson')\n",
    "    \n",
    "    # creates the minimum set of rasters with non-overlapping polygons\n",
    "    wbt.unnest_basins(\n",
    "        fdir_path, \n",
    "        ppt_batch_path, \n",
    "        temp_basin_raster_path,\n",
    "        esri_pntr=False, \n",
    "        # callback=default_callback\n",
    "    )\n",
    "    \n",
    "    batch_rasters = sorted([e for e in sorted(os.listdir(temp_dir)) if e.endswith('.tif')])\n",
    "\n",
    "    # retrieve the batch pour point as a dataframe\n",
    "    ppt_batch = gpd.read_file(ppt_batch_path)\n",
    "\n",
    "    tb1 = time.time()\n",
    "\n",
    "    print(f'    Basins delineated for {region} ppt batch {batch_no} in {(tb1-t_batch_start)/60:.1f}min. {len(batch_rasters)} raster sub-batches to process ({len(ppt_batch)}).')\n",
    "\n",
    "    # process the raster batches in parallel\n",
    "    crs_array = [f'EPSG:3005'] * len(batch_rasters)\n",
    "    # min_A_array = [min_basin_area] * len(batch_raster_fpaths)\n",
    "    resolution_array = [raster_resolution] * len(batch_rasters)\n",
    "    temp_folder_array = [temp_dir] * len(batch_rasters)\n",
    "    min_areas = [min_basin_area] * len(batch_rasters)\n",
    "    path_inputs = list(zip(batch_rasters, crs_array, resolution_array, min_areas, temp_folder_array))\n",
    "    \n",
    "    batch_size_GB = len(batch_rasters) * filesize / 1E3\n",
    "    \n",
    "    # Each processor core will load and process a full size DEM, and\n",
    "    # total ram usage can be multiples of the raster file size for each process. \n",
    "    # You need to balance the number of cores with RAM usage.\n",
    "    print(f'     Estimated batch size = {batch_size_GB} GB')\n",
    "    n_procs = 6\n",
    "    print(f'    Setting n_procs={n_procs:.0f} For a batch size of {batch_size_GB:.1f}GB')\n",
    "\n",
    "    p = mp.Pool(n_procs)\n",
    "    trv0 = time.time()\n",
    "    all_polygons = p.map(raster_to_vector_basins_batch, path_inputs)\n",
    "    trv1 = time.time()\n",
    "    print(f'    {trv1-trv0:.2f}s to convert {len(path_inputs)} rasters to vector basins.')\n",
    "    \n",
    "    # concatenate polygons\n",
    "    trc0 = time.time()\n",
    "    batch_polygons = gpd.GeoDataFrame(pd.concat(all_polygons), crs=region_raster_crs)\n",
    "    \n",
    "    # sort by VALUE to maintain ordering from ppt batching / raster basin unnesting operation\n",
    "    batch_polygons.sort_values(by='VALUE', inplace=True)\n",
    "    batch_polygons.reset_index(inplace=True, drop=True)\n",
    "    batch_polygons.to_file(batch_output_fpath)\n",
    "    trc1 = time.time()\n",
    "    print(f'    {trc1-trc0:.2f}s to concatenate basin vector polygons.')\n",
    "\n",
    "    basin_output_files.append(batch_output_fpath)\n",
    "    \n",
    "    \n",
    "    # remove all temporary files (don't delete the raster batches yet!)\n",
    "    clean_up_temp_files(temp_dir, batch_rasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd56a6c-2cec-4e0f-9134-5ad23d7e21db",
   "metadata": {},
   "source": [
    "## Concatenate Batch Polygon Files into a single Geopackage (optional)\n",
    "\n",
    "You can work with the output geojson files directly (smaller files), or concatenat them and convert to other useful formats like [Geopackage](https://mapscaping.com/understanding-geopackage/#:~:text=Versatility%3A%20GeoPackage%20can%20store%20multiple,spatial%20queries%20and%20data%20visualization.) or [Parquet](https://getindata.com/blog/introducing-geoparquet-data-format/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997f541-2c0d-4359-95b0-ac05a058a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_files = sorted(list(set(basin_output_files)))\n",
    "\n",
    "batch_dfs = []\n",
    "for file in basin_files:\n",
    "    fpath = os.path.join(temp_folder, file)\n",
    "    layer = gpd.read_file(fpath)\n",
    "    batch_dfs.append(layer)\n",
    "    \n",
    "all_data = gpd.GeoDataFrame(pd.concat(batch_dfs), crs=layer.crs)\n",
    "all_data.to_file(output_fpath)\n",
    "all_data.to_file(\n",
    "    output_fpath.replace('.geojson', '.gpkg'), \n",
    "    driver='GPKG', \n",
    "    layer=f'pour_points')\n",
    "    \n",
    "for f in list(set(files)):\n",
    "    os.remove(os.path.join(temp_folder, f))\n",
    "t_n = time.time()\n",
    "n_processed_basins = max(1, n_processed_basins)\n",
    "ut = (t_n - t0) / n_processed_basins\n",
    "print(f'Total processing time for {region}: {t_n-t0:.1f}s ({ut:.2f}/basin).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3857f9-b632-412f-a47f-7c9d8dcf632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
