{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pour Point Extraction\n",
    "\n",
    "```{figure} img/pour_points.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "In this notebook, we will derive a set of pour points describing confluences that will be used to derive basins and extract basin attributes.\n",
    "```\n",
    "\n",
    "In this notebook, we'll use the stream network generated in the previous notebook to find all river confluences.   A set of input pour points are required for basin delineation in the next notebook.\n",
    "\n",
    "The set of confluences will be filtered using the [National Hydrographic Network](https://natural-resources.canada.ca/science-and-data/science-and-research/earth-sciences/geography/topographic-information/geobase-surface-water-program-geeau/national-hydrographic-network/21361) waterbodies geometry to remove spurious confluences within lakes.\n",
    "\n",
    "The remaining points will serve as pour points for basin delineation.  \n",
    "\n",
    "The following files were pre-processed for the purpose of demonstration since the [original files cover all of Canada and are as a result very large](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/gpkg_en/CA/).  The files below (may) need to be downloaded and saved to `content/notebooks/data/region_polygons/`.  \n",
    "\n",
    "* `Vancouver_Island.geojson`: this is the polygon describing Vancouver Island.  It was used to do a spatial intersection on the NHN geometry to select just the waterbody geometries on Vancouver Island.\n",
    "* `Vancouver_Island_lakes.geojson`: the water bodies polygon set for Vancouver Island.\n",
    "\n",
    "\n",
    "The steps in this notebook produce a set of river confluences, with spurious points within lakes filtered out.  The example below shows green points (confluences) and spurious lake points removed (shown yellow for illustration).\n",
    "\n",
    "\n",
    "```{note}\n",
    "Note that the stream network in the image above appears discontinuous due to the screen resolution.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/danbot/Documents/code/23/bcub/content'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from utilities import *\n",
    "# open the stream layer\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "dem_folder = os.path.join(base_dir, 'notebooks/data/DEM/')\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For clarity, some functions have been relegated to a separate file.  To find more detail, see `utilities.py`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder where the pour point geometry information will be saved.\n",
    "pour_pt_path = os.path.join(base_dir, f'notebooks/data/pour_points/')\n",
    "if not os.path.exists(pour_pt_path):\n",
    "    os.mkdir(pour_pt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import rasters (flow direction, accumulation, stream network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster resolution is 22x22m\n"
     ]
    }
   ],
   "source": [
    "# open the streams dem\n",
    "region = 'Vancouver_Island'\n",
    "d8_path = os.path.join(dem_folder, f'{region}_d8_pointer.tif')\n",
    "acc_path = os.path.join(dem_folder, f'{region}_acc.tif')\n",
    "stream_path = os.path.join(dem_folder, f'{region}_streams.tif')\n",
    "\n",
    "stream_raster, stream_crs, affine = retrieve_raster(stream_path)\n",
    "resolution = stream_raster.rio.resolution()\n",
    "dx, dy = abs(resolution[0]), abs(resolution[1])\n",
    "print(f'Raster resolution is {dx:.0f}x{dy:.0f}m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set a minimum threshold of 5 $km^2$ to limit the number of confluences for the sake of this demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_basin_area = 5 # km^2\n",
    "# min number of cells comprising a basin\n",
    "basin_threshold = int(min_basin_area * 1E6 / (dx * dy)) \n",
    "basin_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...time to load resources: 8.5s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rt0 = time.time()\n",
    "\n",
    "stream, _, _ = retrieve_raster(stream_path)\n",
    "fdir, _, _ = retrieve_raster(d8_path)\n",
    "acc, _, _ = retrieve_raster(acc_path)\n",
    "\n",
    "# get raster data in matrix form\n",
    "S = stream.data[0]\n",
    "F = fdir.data[0]\n",
    "A = acc.data[0]\n",
    "\n",
    "rt1 = time.time()\n",
    "print(f'   ...time to load resources: {rt1-rt0:.1f}s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of coordinates representing all the stream cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the stream pixel indices\n",
    "stream_px = np.argwhere(S == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define confluence points in the stream network\n",
    "\n",
    "Below we create a dictionary of potential pour points corresponding to confluences.  \n",
    "\n",
    "We iterate through all the stream pixels, retrieve a 3x3 window of flow direction raster around each one, and check if it has more than one stream cell pointing towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppts = {}\n",
    "nn = 0\n",
    "\n",
    "for (i, j) in stream_px:\n",
    "    c_idx = f'{i},{j}'\n",
    "    if c_idx not in ppts:\n",
    "        ppts[c_idx] = {}\n",
    "    ppt = ppts[c_idx]\n",
    "\n",
    "    # Add river outlets, as these are by definition\n",
    "    # confluences and especially prevalent in coastal regions\n",
    "    focus_cell_acc = A[i, j]\n",
    "    focus_cell_dir = F[i, j]\n",
    "\n",
    "    ppt['acc'] = focus_cell_acc\n",
    "\n",
    "    if focus_cell_dir == 0:\n",
    "        # the focus cell is already defined as a stream cell\n",
    "        # so if its direction value is nan or 0, \n",
    "        # there is no flow direction and it's an outlet cell.\n",
    "        ppt['OUTLET'] = True\n",
    "        # by definition an outlet cell is also a confluence\n",
    "        ppt['CONF'] = True\n",
    "    else:\n",
    "        ppt['OUTLET'] = False\n",
    "\n",
    "    # get the 3x3 boolean matrix of stream and d8 pointer \n",
    "    # cells centred on the focus cell\n",
    "    S_w = S[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    F_w = F[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    \n",
    "    # create a boolean matrix for cells that flow into the focal cell\n",
    "    F_m = mask_flow_direction(S_w, F_w)\n",
    "    \n",
    "    # check if cell is a stream confluence\n",
    "    # set the target cell to false by default\n",
    "    ppts = check_for_confluence(i, j, ppts, S_w, F_m)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionary of stream confluences to a geodataframe in the same CRS as our raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 22560 confluences and outlets combined in the Vancouver_Island region.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "output_ppt_path = os.path.join(pour_pt_path, f'{region}_ppts.geojson')\n",
    "\n",
    "if not os.path.exists(output_ppt_path):\n",
    "    t0 = time.time()\n",
    "    ppt_df = pd.DataFrame.from_dict(ppts, orient='index')\n",
    "    ppt_df.index.name = 'cell_idx'\n",
    "    ppt_df.reset_index(inplace=True) \n",
    "    \n",
    "    # split the cell indices into columns and convert str-->int\n",
    "    ppt_df['ix'] = [int(e.split(',')[0]) for e in ppt_df['cell_idx']]\n",
    "    ppt_df['jx'] = [int(e.split(',')[1]) for e in ppt_df['cell_idx']]\n",
    "    \n",
    "    # filter for stream points that are an outlet or a confluence\n",
    "    ppt_df = ppt_df[(ppt_df['OUTLET'] == True) | (ppt_df['CONF'] == True)]\n",
    "    print(f' There are {len(ppt_df)} confluences and outlets combined in the {region} region.')\n",
    "else:\n",
    "    ppt_df = gpd.read_file(output_ppt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 934567 total stream cells:\n",
      "    21044 (2.4%) are stream confluences,\n",
      "    1516 (0.2%) are stream outlets.\n"
     ]
    }
   ],
   "source": [
    "n_pts_tot = len(stream_px)\n",
    "n_pts_conf = len(ppt_df[ppt_df['CONF']])\n",
    "n_pts_outlet = len(ppt_df[ppt_df['OUTLET']])\n",
    "\n",
    "print(f'Of {n_pts_tot} total stream cells:')\n",
    "print(f'    {n_pts_conf - n_pts_outlet} ({100*n_pts_conf/n_pts_tot:.1f}%) are stream confluences,')\n",
    "print(f'    {n_pts_outlet} ({100*n_pts_outlet/n_pts_tot:.1f}%) are stream outlets.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The pour points are thus far only described by the raster pixel index, we still need to apply a transform to map indices to projected coordinates.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating 100 chunks for processing\n",
      "    ...10/100 chunks processed in 0.0s\n",
      "    ...20/100 chunks processed in 0.1s\n",
      "    ...30/100 chunks processed in 0.1s\n",
      "    ...40/100 chunks processed in 0.2s\n",
      "    ...50/100 chunks processed in 0.2s\n",
      "    ...60/100 chunks processed in 0.2s\n",
      "    ...70/100 chunks processed in 0.3s\n",
      "    ...80/100 chunks processed in 0.3s\n",
      "    ...90/100 chunks processed in 0.3s\n",
      "    ...100/100 chunks processed in 0.4s\n",
      "    22560 pour points created.\n",
      "   ...ppts geodataframe processed in0.4s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppt_gdf = create_pour_point_gdf(region, stream, ppt_df, stream_crs, output_ppt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = time.time()\n",
    "# polygon_path = os.path.join(base_dir, f'notebooks/data/region_polygons/{region}.geojson')\n",
    "# region_polygon = gpd.read_file(polygon_path)\n",
    "# # reproject to match nhn crs\n",
    "# # region_polygon = region_polygon.to_crs(4617)\n",
    "# tb = time.time()\n",
    "# print(f'   ...region polygon opened in {tb-ta:.2f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter spurious confluences\n",
    "\n",
    "\n",
    "One vestige of the stream network derivation is that it does not identify lakes.  There are lots of lakes on Vancouver Island, and we want to remove the spurious confluence points that fall within lakes and find locations where rivers flow into lakes.  We can do this with hydrographic information from the [National Hydrographic Netowork](https://natural-resources.canada.ca/science-and-data/science-and-research/earth-sciences/geography/topographic-information/geobase-surface-water-program-geeau/national-hydrographic-network/21361).\n",
    "\n",
    "```{tip}\n",
    "Lake polygons for Vancouver Island are saved under `content/notebooks/data/region_polygons/Vancouver_Island_lakes.geojson`\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the water body geometries that contain confluence points\n",
    "\n",
    "From the [NHN documentation](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/doc/GeoBase_nhn_en_Catalogue_1_2.pdf):\n",
    "\n",
    "Permanency code:\n",
    "* -1 unknown\n",
    "* 0 no value available\n",
    "* 1 permanent\n",
    "* 2 intermittent\n",
    "\n",
    "    \n",
    "| water_definition | Label | Code Definition |\n",
    "|------------------|-------|-----------------|\n",
    "| None | 0 | No Waterbody Type value available. |\n",
    "| Canal | 1 | An artificial watercourse serving as a navigable waterway or to channel water. |\n",
    "| Conduit | 2 | An artificial system, such as an Aqueduct, Penstock, Flume, or Sluice, designed to carry water for purposes other than drainage. |\n",
    "| Ditch | 3 | Small, open manmade channel constructed through earth or rock for the purpose of conveying water. |\n",
    "| *Lake | 4 | An inland body of water of considerable area. |\n",
    "| *Reservoir | 5 | A wholly or partially manmade feature for storing and/or regulating and controlling water. |\n",
    "| Watercourse | 6 | A channel on or below the earth's surface through which water may flow. |\n",
    "| Tidal River | 7 | A river in which flow and water surface elevation are affected by the tides. |\n",
    "| *Liquid Waste | 8 | Liquid waste from an industrial complex. |\n",
    "\n",
    "```{warning}\n",
    "The label \"10\" also exists, though I have not found a corresponding definition.  From the image below, it appears they may represent seasonal channels.  Light blue regions are lakes (4) and watercourses (6).\n",
    "```\n",
    "\n",
    "```{figure} img/label_10.png\n",
    "---\n",
    "width: 400px\n",
    "---\n",
    "Darker grey polygons are labeled with the code \"10\" appear to be seasonal channels.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pre-processed lakes polygon file\n",
    "region_lakes_path = os.path.join(base_dir, f'notebooks/data/region_polygons/{region}_lakes.geojson')\n",
    "lakes_df = gpd.read_file(region_lakes_path)\n",
    "lakes_df = lakes_df[[c for c in lakes_df.columns if c not in ['index_right', 'index_left']]]\n",
    "assert lakes_df.crs == ppt_gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Here we apply some subjective criteria to improve the performance of the lake inflow point discovery:\n",
    "1. We remove lakes smaller than 0.01 $km^2$ to speed up the spatial join.\n",
    "2. We only look at lakes that contain confluence points in order to relocate points to river mouths.\n",
    "3. We apply a small buffer and simplify (or smooth) each water body polygon -- this is to reduce the number of river mouth points identified in heavily braided lake headwaters. \n",
    "4. Check that points identified as river mouths aren't in too close proximity (within 4 pixels).\n",
    "5. Rasterize the lake polygons in order to find the nearest stream pixel crossing the line -- if we interpolate too few points, we miss the intersecting point.  When changing these parameters, consider that the simplification eliminates vertices defining the polygon, so you must interpolate the line with enough points to find a stream pixel at the intersection.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lakes < 0.01 km^2\n",
    "lakes_df['area'] = lakes_df.geometry.area\n",
    "lakes_df = lakes_df[lakes_df['area'] >= 10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out points within water bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19184/22560 confluence points are not in lakes (3376 points removed).\n"
     ]
    }
   ],
   "source": [
    "# filter for water_definition code (see code table above)\n",
    "# filter out all confluence points in lakes and reservoirs\n",
    "lakes_filter = (lakes_df['water_definition'] == 4) | (lakes_df['water_definition'] == 5) \n",
    "lakes_df = lakes_df[lakes_filter].copy()\n",
    "\n",
    "# intersect the pour point and filtered lake geometries\n",
    "lake_ppts = gpd.sjoin(ppt_gdf, lakes_df, how='left', predicate='within')\n",
    "filtered_ppts = lake_ppts[lake_ppts['index_right'].isna()].copy()\n",
    "\n",
    "print(f'    {len(filtered_ppts)}/{len(ppt_gdf)} confluence points are not in lakes ({len(ppt_gdf) - len(filtered_ppts)} points removed).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all water body polygons that contain at least one confluence point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  There are 311 unique lake geometries that contain confluence points found in the Vancouver_Island waterbodies layer\n"
     ]
    }
   ],
   "source": [
    "# find the set of lakes that contain points\n",
    "lakes_with_pts = gpd.sjoin(lakes_df, ppt_gdf, how='left', predicate='intersects')\n",
    "\n",
    "# the rows with index_right == nan are lake polygons containing no points\n",
    "filtered_lakes = lakes_with_pts[~lakes_with_pts['index_right'].isna()].copy()\n",
    "# # get the set of all unique lake ids\n",
    "lake_ids = list(set(filtered_lakes['id']))\n",
    "filtered_lakes = lakes_df[lakes_df['id'].isin(lake_ids)].copy()\n",
    "\n",
    "# # merge contiguous (adjacent) polygons \n",
    "filtered_lakes = gpd.GeoDataFrame(geometry=[filtered_lakes.geometry.unary_union], crs='EPSG:3005')\n",
    "filtered_lakes = filtered_lakes.explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "print(f'  There are {len(filtered_lakes)} unique lake geometries that contain confluence points found in the {region} waterbodies layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and add lake inflows\n",
    "\n",
    "We'll only check lakes that have spurious confluences, the general idea is we shift the in-lake confluence to the inflow location.  The method works best for large lake polygons and relatively smooth geometries where the stream network and NHN features align well, but it adds unnecessary points in other locations.  A few examples of good and bad behaviour are shown below.  \n",
    "\n",
    "```{figure} img/lake_points_removed.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "Confluence points within lakes have been removed, while river mouths have been added.\n",
    "```\n",
    "\n",
    "```{figure} img/problem_points.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "Complex lake polygons and derived stream network disagreement result in points being added in unintentional locations.\n",
    "```\n",
    "\n",
    "The geometric manipulations below can be modified to address specific use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing lake group 50/311, 298 points so far...\n",
      "   Processing lake group 100/311, 582 points so far...\n",
      "   Processing lake group 150/311, 942 points so far...\n",
      "   Processing lake group 200/311, 1285 points so far...\n",
      "   Processing lake group 250/311, 1741 points so far...\n",
      "   Processing lake group 300/311, 2011 points so far...\n",
      "2078 points identified as potential lake inflows\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "tot_pts = 0\n",
    "tb = time.time()\n",
    "resolution = abs(acc.rio.resolution()[0])\n",
    "min_acc_cells = 1E6 / (resolution**2)\n",
    "# simplify the lake geometry to \n",
    "points_to_check = []\n",
    "for _, row in filtered_lakes.iterrows():\n",
    "    n += 1\n",
    "    if n % 50 == 0:\n",
    "        print(f'   Processing lake group {n}/{len(filtered_lakes)}, {tot_pts} points so far...')\n",
    "    \n",
    "    # give a slight buffer to the polygon and smooth it to address \n",
    "    # complex braided lake headwaters\n",
    "    lake_geom = row.geometry.buffer(resolution).simplify(resolution)\n",
    "    # we may have created multipolygons in the smoothing step, \n",
    "    # # below we keep the main polygon and drop the remainder\n",
    "    # if lake_geom.geom_type == 'MultiPolygon':\n",
    "    #     lake_geom = gpd.GeoDataFrame(geometry=[lake_geom], crs='EPSG:3005')\n",
    "    #     lake_geom = lake_geom.explode(index_parts=False)\n",
    "    #     lake_geom.reset_index(inplace=True, drop=True)\n",
    "    #     lake_geom['area_1'] = lake_geom.area\n",
    "    #     lake_geom = lake_geom.loc[lake_geom['area_1'].idxmax(), :].geometry    \n",
    "    \n",
    "    # if not lake_geom:\n",
    "    #     continue\n",
    "    # resample the shoreline vector to prevent missing confluence points\n",
    "    resampled_shoreline = redistribute_vertices(lake_geom.exterior, resolution).coords.xy\n",
    "    \n",
    "    xs = resampled_shoreline[0].tolist()\n",
    "    ys = resampled_shoreline[1].tolist()\n",
    "\n",
    "    # # find the closest point to within 1/2 pixel of the lake edge\n",
    "    px_pts = acc.sel(x=xs, y=ys, method='nearest', tolerance=resolution/2)\n",
    "    latlon = list(set(zip(px_pts.x.values, px_pts.y.values)))\n",
    "\n",
    "    if len(latlon) == 0:\n",
    "        continue\n",
    "\n",
    "    for x, y in latlon:\n",
    "        acc_val = acc.sel(x=x, y=y).squeeze()\n",
    "        if (acc_val.item() > min_acc_cells):\n",
    "            tot_pts += 1\n",
    "            points_to_check += [(x, y, resolution)]\n",
    "            \n",
    "        \n",
    "print(f'{len(points_to_check)} points identified as potential lake inflows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/2078 points checked.\n",
      "500/2078 points checked.\n",
      "750/2078 points checked.\n",
      "1000/2078 points checked.\n",
      "1250/2078 points checked.\n",
      "1500/2078 points checked.\n",
      "1750/2078 points checked.\n",
      "2000/2078 points checked.\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "all_pts = []\n",
    "for inp in points_to_check:\n",
    "    n += 1\n",
    "    if n % 250 == 0:\n",
    "        print(f'{n}/{len(points_to_check)} points checked.')\n",
    "\n",
    "    x, y, resolution = inp\n",
    "    pt = Point(x, y)\n",
    "    \n",
    "    # index_right is the lake id the point is contained in\n",
    "    # don't let adjacent points both be pour points\n",
    "    # but avoid measuring distance to points within lakes\n",
    "    pt_dists = filtered_ppts[filtered_ppts['index_right'].isna()].distance(pt).min()\n",
    "\n",
    "    # check the point is not within 5 cell widths of an existing point\n",
    "    min_spacing = 10 * resolution\n",
    "    dist_check = pt_dists <= min_spacing\n",
    "    \n",
    "    # accum_check = accum < 0.95 * max_acc\n",
    "    accum_check = True\n",
    "    if accum_check & (~dist_check):\n",
    "        # check if the potential point is in any of the lakes\n",
    "        # not_in_any_lake = sum([lg.contains(pt) for lg in lakes_df.geometry]) == 0\n",
    "        if not lakes_df.contains(pt).any():\n",
    "            all_pts.append(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the river mouth points into a geodataframe and append it to the filtered set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/1668\n",
      "400/1668\n",
      "600/1668\n",
      "800/1668\n",
      "1000/1668\n",
      "1200/1668\n",
      "1400/1668\n",
      "1600/1668\n"
     ]
    }
   ],
   "source": [
    "rpts = filtered_ppts[['geometry']].copy()\n",
    "all_pts_filtered = []\n",
    "n = 0\n",
    "for pt in all_pts:\n",
    "    n += 1\n",
    "    if n % 200 == 0:\n",
    "        print(f'{n}/{len(all_pts)}')\n",
    "    dists = rpts.distance(pt)\n",
    "    if (dists > min_spacing).all():\n",
    "        ptg = gpd.GeoDataFrame(geometry=[pt], crs='EPSG:3005')\n",
    "        # append the new point to the reference point dataframe to\n",
    "        # update the set of points checked against.\n",
    "        rpts = gpd.GeoDataFrame(pd.concat([rpts, ptg]), crs='EPSG:3005')\n",
    "        all_pts_filtered.append(pt)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pts = gpd.GeoDataFrame(geometry=all_pts_filtered, crs=f'EPSG:{stream_crs}')\n",
    "pour_points = gpd.GeoDataFrame(pd.concat([filtered_ppts, new_pts], axis=0), crs=f'EPSG:{stream_crs}')\n",
    "pour_points.to_file(os.path.join(base_dir, f'notebooks/data/pour_points/{region}_pour_points3.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
