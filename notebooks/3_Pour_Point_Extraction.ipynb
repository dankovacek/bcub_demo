{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour Point Extraction\n",
    "\n",
    "```{figure} img/pour_points.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "In this notebook, we will derive a set of pour points describing river confluences.  These points will be used to delineate basins and extract basin attributes.\n",
    "```\n",
    "\n",
    "In this notebook, we'll use the stream network generated in the previous notebook to find all river confluences.  The set of confluences will be filtered using the lake geometries found in the [HydroSHEDS dataset](https://www.hydrosheds.org/page/hydrolakes) geometry to remove spurious confluences within lakes.  The remaining points will serve as input for basin delineation.  \n",
    "\n",
    "The following files were pre-processed for the purpose of demonstration since the [original files cover all of Canada and are as a result very large](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/gpkg_en/CA/).  The files below (may) need to be downloaded and saved to `content/notebooks/data/region_polygons/`.  \n",
    "\n",
    "* `Vancouver_Island.geojson`: this is the polygon describing Vancouver Island.  It was used to capture just the waterbody geometries on Vancouver Island.\n",
    "* `Vancouver_Island_lakes.geojson`: the water bodies polygon set for Vancouver Island.\n",
    "\n",
    "\n",
    "```{figure} img/filtered_pts_example.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "The steps in this notebook produce a set of river confluences (blue), with spurious points within lakes (red) filtered out.  Lake boundaries are traversed to find lake inflows.  \n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utilities import *\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import multiprocessing as mp\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# open the stream layer\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "dem_folder = os.path.join(base_dir, 'notebooks/data/processed_dem/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For clarity, some functions have been relegated to a separate file.  To find more detail, see `utilities.py`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder where the pour point geometry information will be saved.\n",
    "pour_pt_path = os.path.join(base_dir, f'notebooks/data/pour_points/')\n",
    "if not os.path.exists(pour_pt_path):\n",
    "    os.mkdir(pour_pt_path)\n",
    "\n",
    "# import the polygon describing Vancouver Island\n",
    "region_polygon = gpd.read_file('data/region_polygons/Vancouver_Island.geojson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import rasters (flow direction, accumulation, stream network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the streams dem\n",
    "region = 'Vancouver_Island'\n",
    "d8_path = os.path.join(dem_folder, f'{region}_d8_pointer.tif')\n",
    "acc_path = os.path.join(dem_folder, f'{region}_acc.tif')\n",
    "stream_path = os.path.join(dem_folder, f'{region}_streams.tif')\n",
    "# stream_link_path = os.path.join(dem_folder, f'{region}_stream_links.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set a minimum threshold of 5 $km^2$ to limit the number of confluences for the sake of this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster resolution is 22x22m\n",
      "   ...time to load resources: 14.1s.\n"
     ]
    }
   ],
   "source": [
    "rt0 = time.time()\n",
    "\n",
    "stream, _, _ = retrieve_raster(stream_path)\n",
    "# stream_links, _, _ = retrieve_raster(stream_link_path)\n",
    "fdir, _, _ = retrieve_raster(d8_path)\n",
    "acc, _, _ = retrieve_raster(acc_path)\n",
    "\n",
    "resolution = stream.rio.resolution()\n",
    "dx, dy = abs(resolution[0]), abs(resolution[1])\n",
    "print(f'Raster resolution is {dx:.0f}x{dy:.0f}m')\n",
    "\n",
    "# get raster data in matrix form\n",
    "S = stream.data[0]\n",
    "F = fdir.data[0]\n",
    "A = acc.data[0]\n",
    "\n",
    "stream_crs = stream.rio.crs.to_epsg()\n",
    "\n",
    "rt1 = time.time()\n",
    "print(f'   ...time to load resources: {rt1-rt0:.1f}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_basin_area = 5 # km^2\n",
    "# min number of cells comprising a basin\n",
    "basin_threshold = int(min_basin_area * 1E6 / (dx * dy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of coordinates representing all the stream cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the stream pixel indices\n",
    "stream_px = np.argwhere(S == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define confluence points in the stream network\n",
    "\n",
    "Below we create a dictionary of potential pour points corresponding to confluences.  \n",
    "\n",
    "We iterate through all the stream pixels, retrieve a 3x3 window of flow direction raster around each one, and check if it has more than one stream cell pointing towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppts = {}\n",
    "nn = 0\n",
    "\n",
    "for (i, j) in stream_px:\n",
    "    c_idx = f'{i},{j}'\n",
    "    if c_idx not in ppts:\n",
    "        ppts[c_idx] = {}\n",
    "    ppt = ppts[c_idx]\n",
    "\n",
    "    # Add river outlets\n",
    "    focus_cell_acc = A[i, j]\n",
    "    focus_cell_dir = F[i, j]\n",
    "\n",
    "    ppt['acc'] = focus_cell_acc\n",
    "\n",
    "    if focus_cell_dir == 0:\n",
    "        # the focus cell is already defined as a stream cell\n",
    "        # so if its direction value is nan or 0, \n",
    "        # there is no flow direction and it's an outlet cell.\n",
    "        ppt['OUTLET'] = True\n",
    "        # by definition an outlet cell is also a confluence\n",
    "        ppt['CONF'] = True\n",
    "    else:\n",
    "        ppt['OUTLET'] = False\n",
    "\n",
    "    # get the 3x3 boolean matrix of stream and d8 pointer \n",
    "    # cells centred on the focus cell\n",
    "    S_w = S[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    F_w = F[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    \n",
    "    # create a boolean matrix for cells that flow into the focal cell\n",
    "    F_m = mask_flow_direction(S_w, F_w)\n",
    "    \n",
    "    # check if cell is a stream confluence\n",
    "    # set the target cell to false by default\n",
    "    ppts = check_for_confluence(i, j, ppts, S_w, F_m)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionary of stream confluences to a geodataframe in the same CRS as our raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/danbot/Documents/code/23/bcub_demo/notebooks/data/pour_points/Vancouver_Island_ppt.geojson\n",
      " There are 22556 confluences and outlets combined in the Vancouver_Island region.\n"
     ]
    }
   ],
   "source": [
    "output_ppt_path = os.path.join(pour_pt_path, f'{region}_ppt.geojson')\n",
    "print(output_ppt_path)\n",
    "if not os.path.exists(output_ppt_path):\n",
    "    t0 = time.time()\n",
    "    ppt_df = pd.DataFrame.from_dict(ppts, orient='index')\n",
    "    ppt_df.index.name = 'cell_idx'\n",
    "    ppt_df.reset_index(inplace=True) \n",
    "    \n",
    "    # split the cell indices into columns and convert str-->int\n",
    "    ppt_df['ix'] = [int(e.split(',')[0]) for e in ppt_df['cell_idx']]\n",
    "    ppt_df['jx'] = [int(e.split(',')[1]) for e in ppt_df['cell_idx']]\n",
    "    \n",
    "    # filter for stream points that are an outlet or a confluence\n",
    "    ppt_df = ppt_df[(ppt_df['OUTLET'] == True) | (ppt_df['CONF'] == True)]\n",
    "    print(f' There are {len(ppt_df)} confluences and outlets combined in the {region} region.')\n",
    "else:\n",
    "    print('existing file')\n",
    "    ppt_df = gpd.read_file(output_ppt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 934639 total stream cells:\n",
      "    21041 (2.4%) are stream confluences,\n",
      "    1515 (0.2%) are stream outlets.\n"
     ]
    }
   ],
   "source": [
    "n_pts_tot = len(stream_px)\n",
    "n_pts_conf = len(ppt_df[ppt_df['CONF']])\n",
    "n_pts_outlet = len(ppt_df[ppt_df['OUTLET']])\n",
    "\n",
    "print(f'Of {n_pts_tot} total stream cells:')\n",
    "print(f'    {n_pts_conf - n_pts_outlet} ({100*n_pts_conf/n_pts_tot:.1f}%) are stream confluences,')\n",
    "print(f'    {n_pts_outlet} ({100*n_pts_outlet/n_pts_tot:.1f}%) are stream outlets.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The pour points are thus far only described by the raster pixel index, we still need to apply a transform to map indices to projected coordinates.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating 100 chunks for processing\n",
      "    ...10/100 chunks processed in 0.0s\n",
      "    ...20/100 chunks processed in 0.1s\n",
      "    ...30/100 chunks processed in 0.1s\n",
      "    ...40/100 chunks processed in 0.1s\n",
      "    ...50/100 chunks processed in 0.2s\n",
      "    ...60/100 chunks processed in 0.2s\n",
      "    ...70/100 chunks processed in 0.2s\n",
      "    ...80/100 chunks processed in 0.2s\n",
      "    ...90/100 chunks processed in 0.3s\n",
      "    ...100/100 chunks processed in 0.3s\n",
      "    22556 pour points created.\n",
      "   ...ppts geodataframe processed in0.3s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_idx</th>\n",
       "      <th>acc</th>\n",
       "      <th>OUTLET</th>\n",
       "      <th>CONF</th>\n",
       "      <th>ix</th>\n",
       "      <th>jx</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>199,4302</td>\n",
       "      <td>22657.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>4302</td>\n",
       "      <td>POINT (865049.001 659747.390)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>214,4329</td>\n",
       "      <td>21779.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>214</td>\n",
       "      <td>4329</td>\n",
       "      <td>POINT (865649.625 659413.710)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>214,4330</td>\n",
       "      <td>18179.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>214</td>\n",
       "      <td>4330</td>\n",
       "      <td>POINT (865671.870 659413.710)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>215,4329</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>215</td>\n",
       "      <td>4329</td>\n",
       "      <td>POINT (865649.625 659391.465)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>234,4352</td>\n",
       "      <td>11530.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>234</td>\n",
       "      <td>4352</td>\n",
       "      <td>POINT (866161.268 658968.803)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_idx      acc  OUTLET  CONF   ix    jx                       geometry\n",
       "9    199,4302  22657.0    True  True  199  4302  POINT (865049.001 659747.390)\n",
       "39   214,4329  21779.0   False  True  214  4329  POINT (865649.625 659413.710)\n",
       "40   214,4330  18179.0   False  True  214  4330  POINT (865671.870 659413.710)\n",
       "41   215,4329   3591.0   False  True  215  4329  POINT (865649.625 659391.465)\n",
       "137  234,4352  11530.0   False  True  234  4352  POINT (866161.268 658968.803)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_gdf = create_pour_point_gdf(region, stream, ppt_df, stream_crs, output_ppt_path)\n",
    "ppt_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter spurious confluences\n",
    "\n",
    "\n",
    "One issue with the stream network algorithm is it does not identify lakes.  There are many lakes on Vancouver Island, and we want to remove the spurious confluence points that fall within lakes, and we want to add points where rivers empty into lakes.  We can use hydrographic information from the [National Hydrographic Netowork](https://natural-resources.canada.ca/science-and-data/science-and-research/earth-sciences/geography/topographic-information/geobase-surface-water-program-geeau/national-hydrographic-network/21361) to do so.\n",
    "\n",
    "```{tip}\n",
    "Lake polygons for Vancouver Island are saved under `content/notebooks/data/region_polygons/Vancouver_Island_lakes.geojson`\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the water body geometries that contain confluence points\n",
    "\n",
    "From the [NHN documentation](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/doc/GeoBase_nhn_en_Catalogue_1_2.pdf):\n",
    "\n",
    "Permanency code:\n",
    "* -1 unknown\n",
    "* 0 no value available\n",
    "* 1 permanent\n",
    "* 2 intermittent\n",
    "\n",
    "    \n",
    "| water_definition | Label | Code Definition |\n",
    "|------------------|-------|-----------------|\n",
    "| None | 0 | No Waterbody Type value available. |\n",
    "| Canal | 1 | An artificial watercourse serving as a navigable waterway or to channel water. |\n",
    "| Conduit | 2 | An artificial system, such as an Aqueduct, Penstock, Flume, or Sluice, designed to carry water for purposes other than drainage. |\n",
    "| Ditch | 3 | Small, open manmade channel constructed through earth or rock for the purpose of conveying water. |\n",
    "| *Lake | 4 | An inland body of water of considerable area. |\n",
    "| *Reservoir | 5 | A wholly or partially manmade feature for storing and/or regulating and controlling water. |\n",
    "| Watercourse | 6 | A channel on or below the earth's surface through which water may flow. |\n",
    "| Tidal River | 7 | A river in which flow and water surface elevation are affected by the tides. |\n",
    "| *Liquid Waste | 8 | Liquid waste from an industrial complex. |\n",
    "\n",
    "```{warning}\n",
    "The label \"10\" also exists, though I have not found a corresponding definition.  From the image below, it appears they may represent seasonal channels.  Light blue regions are lakes (4) and watercourses (6).\n",
    "```\n",
    "\n",
    "```{figure} img/label_10.png\n",
    "---\n",
    "width: 400px\n",
    "---\n",
    "Darker grey polygons are labeled with the code \"10\" appear to be seasonal channels.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the lakes geometry from [HydroLAKES](https://www.hydrosheds.org/products/hydrolakes) and update the path below to reflect where it is saved.  We first need to clip the data to the region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lakes(lakes_df, ppts, resolution):\n",
    "    \"\"\"\n",
    "    Permanency code:\n",
    "    -1 unknown\n",
    "    0 no value available\n",
    "    1 permanent\n",
    "    2 intermittent\n",
    "\n",
    "    Args:\n",
    "        wb_df (geodataframe): Water body geometries.\n",
    "        ppts (geodataframe): Pour points.\n",
    "        \n",
    "    water_definition Label Definition\n",
    "    ----------------------------- ---- ----------\n",
    "    None            0       No Waterbody Type value available.\n",
    "    Canal           1       An artificial watercourse serving as a navigable waterway or to\n",
    "                            channel water.\n",
    "    Conduit         2       An artificial system, such as an Aqueduct, Penstock, Flume, or\n",
    "                            Sluice, designed to carry water for purposes other than\n",
    "                            drainage.\n",
    "    Ditch           3       Small, open manmade channel constructed through earth or\n",
    "                            rock for the purpose of conveying water.\n",
    "    *Lake           4       An inland body of water of considerable area.\n",
    "    *Reservoir      5       A wholly or partially manmade feature for storing and/or\n",
    "                            regulating and controlling water.\n",
    "    Watercourse     6       A channel on or below the earth's surface through which water\n",
    "                            may flow.\n",
    "    Tidal River     7       A river in which flow and water surface elevation are affected by\n",
    "                            the tides.\n",
    "    *Liquid Waste   8       Liquid waste from an industrial complex.\n",
    "    \"\"\"    \n",
    "    lakes_df = lakes_df.to_crs(ppts.crs)\n",
    "    \n",
    "    # reproject to projected CRS before calculating area\n",
    "    lakes_df['area'] = lakes_df.geometry.area\n",
    "    lakes_df['lake_id'] = lakes_df.index.values\n",
    "        \n",
    "    # filter lakes smaller than 0.1 km^2\n",
    "    min_area = 100000\n",
    "    lakes_df = lakes_df[lakes_df['area'] > min_area]\n",
    "    lakes_df = lakes_df.dissolve().explode(index_parts=False).reset_index(drop=True)\n",
    "    lake_cols = lakes_df.columns\n",
    "    \n",
    "    # filter out Point type geometries\n",
    "    lakes_df = lakes_df[~lakes_df.geometry.type.isin(['Point', 'LineString'])]\n",
    "    # find and fill holes in polygons    \n",
    "    lakes_df.geometry = [Polygon(p.exterior) for p in lakes_df.geometry]\n",
    "        \n",
    "    # find the set of lakes that contain confluence points\n",
    "    lakes_with_pts = gpd.sjoin(lakes_df, ppts, how='left', predicate='intersects')\n",
    "    \n",
    "    # the rows with index_right == nan are lake polygons containing no points\n",
    "    lakes_with_pts = lakes_with_pts[~lakes_with_pts['index_right'].isna()]\n",
    "    lakes_with_pts = lakes_with_pts[[c for c in lakes_with_pts.columns if 'index_' not in c]]\n",
    "    # drop all duplicate indices\n",
    "    lakes_with_pts = lakes_with_pts[~lakes_with_pts.index.duplicated(keep='first')]\n",
    "    lakes_with_pts.area = lakes_with_pts.geometry.area\n",
    "        \n",
    "    # use negative and positive buffers to remove small \"appendages\"\n",
    "    # that tend to add many superfluous inflow points\n",
    "    distance = 100  # metres\n",
    "    lakes_with_pts.geometry = lakes_with_pts.buffer(-distance).buffer(distance * 1.5).simplify(resolution/np.sqrt(2))\n",
    "    lakes_with_pts['geometry'] = lakes_with_pts.apply(lambda row: trim_appendages(row), axis=1)\n",
    "    return lakes_with_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_appendages(row):\n",
    "    g = gpd.GeoDataFrame(geometry=[row['geometry']], crs='EPSG:3005')\n",
    "    geom = g.explode(index_parts=True)\n",
    "    geom['area'] = geom.geometry.area\n",
    "    if len(geom) > 1:\n",
    "        # return only the largest geometry by area\n",
    "        return geom.loc[geom['area'].idxmax(), 'geometry']\n",
    "    return row['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydroLakes_fpath = '/home/danbot/Documents/code/23/bcub/input_data/BasinATLAS/HydroLAKES_polys_v10.gdb/HydroLAKES_polys_v10.gdb'\n",
    "hydroLakes_clipped_fpath = os.path.join(base_dir, 'notebooks/data/geospatial_layers/HydroLAKES_clipped.gpkg')\n",
    "lakes_df_fpath = os.path.join(base_dir, f'notebooks/data/geospatial_layers/{region}_lakes.geojson')\n",
    "\n",
    "# the HydroLAKES CRS is EPSG:4326\n",
    "lakes_crs = 4326\n",
    "\n",
    "if not os.path.exists(hydroLakes_fpath):\n",
    "    err_msg = f'HydroLAKES file not found at {lakes_fpath}.  Download from https://www.hydrosheds.org/products/hydrolakes.  See README for details.'\n",
    "    raise Exception(err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip and reproject the HydroLAKES layer\n",
    "if not os.path.exists(lakes_df_fpath):\n",
    "    print('    Creating region water bodies layer.')\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # import the NHN water body features\n",
    "    bbox_geom = tuple(region_polygon.to_crs(lakes_crs).bounds.values[0])\n",
    "    lake_features_box = gpd.read_file(hydroLakes_clipped_fpath, \n",
    "                                  bbox=bbox_geom)\n",
    "    \n",
    "    # clip features to the region polygon\n",
    "    region_polygon = region_polygon.to_crs(lake_features_box.crs)\n",
    "    lake_features = gpd.clip(lake_features_box, region_polygon, keep_geom_type=False)\n",
    "    t2 = time.time()\n",
    "    print(f'    Lakes layer opened in {t2-t1:.0f}s')\n",
    "    print(f'    Creating lakes geometry file for {region}')\n",
    "    lakes_df = filter_lakes(lake_features, ppt_gdf, abs(resolution[0]))\n",
    "    lakes_df = lakes_df[~lakes_df.geometry.is_empty]\n",
    "    lakes_df.to_file(lakes_df_fpath)\n",
    "    n_lakes = len(lakes_df)\n",
    "    print(f'    File saved.  There are {n_lakes} water body objects in {region}.')\n",
    "else:\n",
    "    lakes_df = gpd.read_file(lakes_df_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Below we apply some subjective criteria to improve the performance of the lake inflow point discovery:\n",
    "1. Remove lakes smaller than 0.01 $km^2$ to speed up the spatial join.\n",
    "2. Only process lakes that contain confluence points in order to relocate points to river mouths.\n",
    "3. Manipulate the lake polygons to smooth the edges -- Where the stream raster disagrees with the NHN polygons it tends to generate spurious inflow points and this step is to mitigate the issue. \n",
    "4. Require a minimum distance to existing confluence points (> 4 pixels).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_df = lakes_df.to_crs(ppt_gdf.crs)\n",
    "# reproject to projected CRS before calculating area\n",
    "lakes_df['area'] = lakes_df.geometry.area\n",
    "lakes_df['lake_id'] = lakes_df.index.values\n",
    "\n",
    "# filter lakes smaller than 0.01 km^2\n",
    "min_area = 10000\n",
    "lakes_df = lakes_df[lakes_df['area'] > min_area]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify lake geometries\n",
    "\n",
    "Misalignment of the derived stream network and the hydrographic information from the NHN produces spurious points when we try to find streams flowing into lakes.  Simplifying (smoothing) the lake polygons trims long narrow segments classified as lake where feature alignment is most likely to occur.\n",
    "\n",
    "```{figure} img/simplified_polygon.png\n",
    "---\n",
    "width: 400px\n",
    "---\n",
    "A simplified polygon reduces the perimeter of the lake polygon in order to capture where stream lines cross the lake boundary.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_appendages(row):\n",
    "    g = gpd.GeoDataFrame(geometry=[row['geometry']], crs='EPSG:3005')\n",
    "    geom = g.copy().explode(index_parts=True)\n",
    "    geom['area'] = geom.geometry.area\n",
    "    if len(geom) > 1:\n",
    "        # return only the largest geometry by area\n",
    "        largest_geom = geom.loc[geom['area'].idxmax(), 'geometry']\n",
    "        return largest_geom\n",
    "    return row['geometry']\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20627/22556 confluence points are not in lakes (1929 points removed).\n"
     ]
    }
   ],
   "source": [
    "lake_ppts = ppt_gdf.clip(lakes_df)\n",
    "filtered_ppts = ppt_gdf[~ppt_gdf['cell_idx'].isin(lake_ppts['cell_idx'])]\n",
    "print(f'    {len(filtered_ppts)}/{len(ppt_gdf)} confluence points are not in lakes ({len(ppt_gdf) - len(filtered_ppts)} points removed).')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and add lake inflows\n",
    "\n",
    "We'll only check lakes that have spurious confluences, the general idea is we shift the in-lake confluence to the inflow location.  The method works best for large lake polygons and relatively smooth geometries where the stream network and NHN features align well, but it adds unnecessary points in other locations.  A few examples of good and bad behaviour are shown below.  \n",
    "\n",
    "```{figure} img/lake_points_removed.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "Confluence points within lakes have been removed, while river mouths have been added.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link_ids(target):\n",
    "    x, y = target\n",
    "    stream_loc = stream.sel(x=x, y=y).squeeze()\n",
    "    link_id = stream_loc.item()\n",
    "    if ~np.isnan(link_id):\n",
    "        i, j = np.argwhere(stream.x.values == x)[0], np.argwhere(stream.y.values == y)[0]\n",
    "        return [i[0], j[0], Point(x, y), link_id]\n",
    "    else:\n",
    "        nbr = stream.rio.clip_box(x-resolution[0], y-resolution[0], x+resolution[0],y+resolution[0])\n",
    "        \n",
    "        if np.isnan(nbr.data).all():\n",
    "            return None\n",
    "        \n",
    "        raster_nonzero = nbr.where(nbr > 0, drop=True)\n",
    "        \n",
    "        # Check surrounding cells for nonzero link_ids\n",
    "        xs, ys = raster_nonzero.x.values, raster_nonzero.y.values\n",
    "        for x1, y1 in zip(xs, ys):\n",
    "            link_id = nbr.sel(x=x1, y=y1, method='nearest', tolerance=resolution[0]).squeeze().item()\n",
    "            ix, jx = np.argwhere(stream.x.values == x1)[0], np.argwhere(stream.y.values == y1)[0]\n",
    "            \n",
    "            # check if point is valid\n",
    "            if np.isnan(ix) | np.isnan(jx):\n",
    "                print(x, y, xs, ys, link_id)\n",
    "                print(ix, jx)\n",
    "            if ~np.isnan(link_id):\n",
    "                return [ix[0], jx[0], Point(x1, y1), link_id]\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lake_inflows(lakes_df, ppts, stream, acc):\n",
    "    \n",
    "    n = 0\n",
    "    tot_pts = 0\n",
    "    resolution = abs(stream.rio.resolution()[0])\n",
    "    crs = stream.rio.crs.to_epsg()\n",
    "\n",
    "    points_to_check = []\n",
    "    for _, row in lakes_df.iterrows():\n",
    "        n += 1\n",
    "        if n % 10 == 0:\n",
    "            print(f'   Processing lake group {n}/{len(lakes_df)}.')\n",
    "        \n",
    "        lake_geom = row['geometry']\n",
    "        # resample the shoreline vector to prevent missing confluence points\n",
    "        resampled_shoreline = redistribute_vertices(lake_geom.exterior, resolution).coords.xy\n",
    "        xs = resampled_shoreline[0].tolist()\n",
    "        ys = resampled_shoreline[1].tolist()\n",
    "\n",
    "        # find the closest cell to within 1 pixel diagonal of the lake polygon boundary\n",
    "        # this is the problem here.\n",
    "        # what's happening is for each interpolated point on the line, \n",
    "        # we look for the nearest pixel in the stream raster\n",
    "        # we should iterate through and find the nearest *stream pixel* \n",
    "        # and record it if \n",
    "        #           i)  it's not in a lake and \n",
    "        #           ii) not on a stream link already recorded\n",
    "        px_pts = stream.sel(x=xs, y=ys, method='nearest', tolerance=resolution)\n",
    "        latlon = list(set(zip(px_pts.x.values, px_pts.y.values)))\n",
    "        latlon = [e for e in latlon if e is not None]\n",
    "        if len(latlon) == 0:\n",
    "            print('skip')\n",
    "            continue\n",
    "        \n",
    "        # the line interpolation misses some cells,\n",
    "        # so check around each point for stream cells\n",
    "        # that aren't inside the lake polygon\n",
    "        pl = mp.Pool()\n",
    "        results = pl.map(find_link_ids, latlon)\n",
    "        results = [r for r in results if r is not None]\n",
    "        \n",
    "        pl.close()\n",
    "        \n",
    "        pts = pd.DataFrame(results, columns=['ix', 'jx', 'geometry', 'link_id'])\n",
    "        # drop duplicate link_ids\n",
    "        pts['CONF'] = True\n",
    "        pts = pts[~pts['link_id'].duplicated(keep='first')]\n",
    "        pts.dropna(subset='geometry', inplace=True)\n",
    "            \n",
    "        points_to_check += [pts]\n",
    "\n",
    "    return points_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search along the boundary for lake-river confluences\n",
    "\n",
    "This step takes **18 minutes** to process on a six core intel i7-8850H @2.6 GHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing lake group 10/159.\n",
      "   Processing lake group 20/159.\n",
      "   Processing lake group 30/159.\n",
      "   Processing lake group 40/159.\n",
      "   Processing lake group 50/159.\n",
      "   Processing lake group 60/159.\n",
      "   Processing lake group 70/159.\n",
      "   Processing lake group 80/159.\n",
      "   Processing lake group 90/159.\n",
      "   Processing lake group 100/159.\n",
      "   Processing lake group 110/159.\n",
      "   Processing lake group 120/159.\n",
      "   Processing lake group 130/159.\n",
      "   Processing lake group 140/159.\n",
      "   Processing lake group 150/159.\n",
      "    159 points identified as potential lake inflows\n"
     ]
    }
   ],
   "source": [
    "points_to_check = add_lake_inflows(lakes_df, filtered_ppts, stream, acc)\n",
    "print(f'    {len(points_to_check)} points identified as potential lake inflows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that found points are not too close to an existing point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_point_spacing(all_pts_df, ppts, ppt_gdf):\n",
    "    n = 0\n",
    "    all_pts = []\n",
    "    acc_vals = []\n",
    "    for i, row in all_pts_df.iterrows():\n",
    "        n += 1\n",
    "        pt = row['geometry']\n",
    "        if n % 250 == 0:\n",
    "            print(f'{n}/{len(ppts)} points checked.')\n",
    "        \n",
    "        # index_right is the lake id the point is contained in\n",
    "        # don't let adjacent points both be pour points\n",
    "        # but avoid measuring distance to points within lakes\n",
    "        nearest_neighbour = ppts.distance(row['geometry']).min()\n",
    "    \n",
    "        # check the point is not within some distance (in m) of an existing point\n",
    "        # 250m is roughly 10 pixels\n",
    "        min_spacing = 250\n",
    "        # check the point is not within some distance (in m) of an existing point    \n",
    "        if nearest_neighbour > min_spacing:\n",
    "            all_pts.append(i)\n",
    "            x, y = pt.x, pt.y\n",
    "            acc_val = acc.sel(x=x, y=y, method='nearest').item()\n",
    "            acc_vals.append(acc_val)\n",
    "            \n",
    "    all_points = all_points_df.iloc[all_pts].copy()\n",
    "    all_points['acc'] = acc_vals\n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1929 points eliminated (fall within lakes)\n",
      "    134 points added for lake inflows.\n",
      "    20761 points after filter and merge. (1795 difference)\n"
     ]
    }
   ],
   "source": [
    "pts_crs = 3005\n",
    "all_points_df = gpd.GeoDataFrame(pd.concat(points_to_check, axis=0), crs=f'EPSG:{pts_crs}')\n",
    "all_points_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "\n",
    "new_pts = check_point_spacing(all_points_df, filtered_ppts, ppt_gdf)\n",
    "output_ppts = gpd.GeoDataFrame(pd.concat([filtered_ppts, new_pts], axis=0), crs=f'EPSG:{stream.rio.crs.to_epsg()}')\n",
    "n_pts0, n_pts1, n_final = len(ppt_gdf), len(filtered_ppts), len(output_ppts)\n",
    "        \n",
    "print(f'    {n_pts0-n_pts1} points eliminated (fall within lakes)')\n",
    "print(f'    {len(new_pts)} points added for lake inflows.')\n",
    "print(f'    {n_final} points after filter and merge. ({n_pts0-n_final} difference)')\n",
    "output_ppts['region_code'] = region\n",
    "# drop unnecessary labels\n",
    "output_ppts.drop(labels=['cell_idx', 'link_id', 'OUTLET', 'CONF'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the river mouth points into a geodataframe and append it to the filtered set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pts = gpd.GeoDataFrame(geometry=all_pts_filtered, crs=f'EPSG:{stream_crs}')\n",
    "# pour_points = gpd.GeoDataFrame(pd.concat([filtered_ppts, new_pts], axis=0), crs=f'EPSG:{stream_crs}')\n",
    "output_ppts.to_file(os.path.join(base_dir, f'notebooks/data/pour_points/{region}_pour_points_filtered.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
