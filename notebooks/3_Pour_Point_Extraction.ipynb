{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pour Point Extraction\n",
    "\n",
    "```{figure} img/pour_points.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "In this notebook, we will derive a set of pour points describing confluences that will be used to derive basins and extract basin attributes.\n",
    "```\n",
    "\n",
    "In this notebook, we'll use the stream network generated in the previous notebook to find all river confluences.  The set of confluences will be filtered using the [National Hydrographic Network](https://natural-resources.canada.ca/science-and-data/science-and-research/earth-sciences/geography/topographic-information/geobase-surface-water-program-geeau/national-hydrographic-network/21361) waterbodies geometry to remove spurious confluences within lakes.  The remaining points will serve as input for basin delineation.  \n",
    "\n",
    "The following files were pre-processed for the purpose of demonstration since the [original files cover all of Canada and are as a result very large](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/gpkg_en/CA/).  The files below (may) need to be downloaded and saved to `content/notebooks/data/region_polygons/`.  \n",
    "\n",
    "* `Vancouver_Island.geojson`: this is the polygon describing Vancouver Island.  It was used to capture just the waterbody geometries on Vancouver Island.\n",
    "* `Vancouver_Island_lakes.geojson`: the water bodies polygon set for Vancouver Island.\n",
    "\n",
    "\n",
    "```{figure} img/filtered_pts_example.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "The steps in this notebook produce a set of river confluences (blue), with spurious points within lakes (red) filtered out.  Lake boundaries are traversed to find lake inflows.  \n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utilities import *\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import multiprocessing as mp\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# open the stream layer\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "dem_folder = os.path.join(base_dir, 'notebooks/data/processed_dem/')\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For clarity, some functions have been relegated to a separate file.  To find more detail, see `utilities.py`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder where the pour point geometry information will be saved.\n",
    "pour_pt_path = os.path.join(base_dir, f'notebooks/data/pour_points/')\n",
    "if not os.path.exists(pour_pt_path):\n",
    "    os.mkdir(pour_pt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import rasters (flow direction, accumulation, stream network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the streams dem\n",
    "region = 'Vancouver_Island'\n",
    "d8_path = os.path.join(dem_folder, f'{region}_d8_pointer.tif')\n",
    "acc_path = os.path.join(dem_folder, f'{region}_acc.tif')\n",
    "stream_path = os.path.join(dem_folder, f'{region}_streams.tif')\n",
    "stream_link_path = os.path.join(dem_folder, f'{region}_stream_links.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set a minimum threshold of 5 $km^2$ to limit the number of confluences for the sake of this demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt0 = time.time()\n",
    "\n",
    "stream, _, _ = retrieve_raster(stream_path)\n",
    "stream_links, _, _ = retrieve_raster(stream_link_path)\n",
    "fdir, _, _ = retrieve_raster(d8_path)\n",
    "acc, _, _ = retrieve_raster(acc_path)\n",
    "\n",
    "resolution = stream.rio.resolution()\n",
    "dx, dy = abs(resolution[0]), abs(resolution[1])\n",
    "print(f'Raster resolution is {dx:.0f}x{dy:.0f}m')\n",
    "\n",
    "# get raster data in matrix form\n",
    "S = stream.data[0]\n",
    "F = fdir.data[0]\n",
    "A = acc.data[0]\n",
    "\n",
    "stream_crs = stream.rio.crs.to_epsg()\n",
    "\n",
    "rt1 = time.time()\n",
    "print(f'   ...time to load resources: {rt1-rt0:.1f}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_basin_area = 5 # km^2\n",
    "# min number of cells comprising a basin\n",
    "basin_threshold = int(min_basin_area * 1E6 / (dx * dy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of coordinates representing all the stream cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the stream pixel indices\n",
    "stream_px = np.argwhere(S == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define confluence points in the stream network\n",
    "\n",
    "Below we create a dictionary of potential pour points corresponding to confluences.  \n",
    "\n",
    "We iterate through all the stream pixels, retrieve a 3x3 window of flow direction raster around each one, and check if it has more than one stream cell pointing towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppts = {}\n",
    "nn = 0\n",
    "\n",
    "for (i, j) in stream_px:\n",
    "    c_idx = f'{i},{j}'\n",
    "    if c_idx not in ppts:\n",
    "        ppts[c_idx] = {}\n",
    "    ppt = ppts[c_idx]\n",
    "\n",
    "    # Add river outlets, as these are by definition\n",
    "    # confluences and especially prevalent in coastal regions\n",
    "    focus_cell_acc = A[i, j]\n",
    "    focus_cell_dir = F[i, j]\n",
    "\n",
    "    ppt['acc'] = focus_cell_acc\n",
    "\n",
    "    if focus_cell_dir == 0:\n",
    "        # the focus cell is already defined as a stream cell\n",
    "        # so if its direction value is nan or 0, \n",
    "        # there is no flow direction and it's an outlet cell.\n",
    "        ppt['OUTLET'] = True\n",
    "        # by definition an outlet cell is also a confluence\n",
    "        ppt['CONF'] = True\n",
    "    else:\n",
    "        ppt['OUTLET'] = False\n",
    "\n",
    "    # get the 3x3 boolean matrix of stream and d8 pointer \n",
    "    # cells centred on the focus cell\n",
    "    S_w = S[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    F_w = F[max(0, i-1):i+2, max(0, j-1):j+2].copy()\n",
    "    \n",
    "    # create a boolean matrix for cells that flow into the focal cell\n",
    "    F_m = mask_flow_direction(S_w, F_w)\n",
    "    \n",
    "    # check if cell is a stream confluence\n",
    "    # set the target cell to false by default\n",
    "    ppts = check_for_confluence(i, j, ppts, S_w, F_m)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionary of stream confluences to a geodataframe in the same CRS as our raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ppt_path = os.path.join(pour_pt_path, f'{region}_pour_.geojson')\n",
    "print(output_ppt_path)\n",
    "if not os.path.exists(output_ppt_path):\n",
    "    t0 = time.time()\n",
    "    ppt_df = pd.DataFrame.from_dict(ppts, orient='index')\n",
    "    ppt_df.index.name = 'cell_idx'\n",
    "    ppt_df.reset_index(inplace=True) \n",
    "    \n",
    "    # split the cell indices into columns and convert str-->int\n",
    "    ppt_df['ix'] = [int(e.split(',')[0]) for e in ppt_df['cell_idx']]\n",
    "    ppt_df['jx'] = [int(e.split(',')[1]) for e in ppt_df['cell_idx']]\n",
    "    \n",
    "    # filter for stream points that are an outlet or a confluence\n",
    "    ppt_df = ppt_df[(ppt_df['OUTLET'] == True) | (ppt_df['CONF'] == True)]\n",
    "    print(f' There are {len(ppt_df)} confluences and outlets combined in the {region} region.')\n",
    "else:\n",
    "    print('existing file')\n",
    "    ppt_df = gpd.read_file(output_ppt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts_tot = len(stream_px)\n",
    "n_pts_conf = len(ppt_df[ppt_df['CONF']])\n",
    "n_pts_outlet = len(ppt_df[ppt_df['OUTLET']])\n",
    "\n",
    "print(f'Of {n_pts_tot} total stream cells:')\n",
    "print(f'    {n_pts_conf - n_pts_outlet} ({100*n_pts_conf/n_pts_tot:.1f}%) are stream confluences,')\n",
    "print(f'    {n_pts_outlet} ({100*n_pts_outlet/n_pts_tot:.1f}%) are stream outlets.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The pour points are thus far only described by the raster pixel index, we still need to apply a transform to map indices to projected coordinates.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_gdf = create_pour_point_gdf(region, stream, ppt_df, stream_crs, output_ppt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = time.time()\n",
    "# polygon_path = os.path.join(base_dir, f'notebooks/data/region_polygons/{region}.geojson')\n",
    "# region_polygon = gpd.read_file(polygon_path)\n",
    "# # reproject to match nhn crs\n",
    "# # region_polygon = region_polygon.to_crs(4617)\n",
    "# tb = time.time()\n",
    "# print(f'   ...region polygon opened in {tb-ta:.2f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter spurious confluences\n",
    "\n",
    "\n",
    "One issue with the stream network algorithm is it does not identify lakes.  There are many lakes on Vancouver Island, and we want to remove the spurious confluence points that fall within lakes, and we want to add points where rivers empty into lakes.  We can use hydrographic information from the [National Hydrographic Netowork](https://natural-resources.canada.ca/science-and-data/science-and-research/earth-sciences/geography/topographic-information/geobase-surface-water-program-geeau/national-hydrographic-network/21361) to do so.\n",
    "\n",
    "```{tip}\n",
    "Lake polygons for Vancouver Island are saved under `content/notebooks/data/region_polygons/Vancouver_Island_lakes.geojson`\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the water body geometries that contain confluence points\n",
    "\n",
    "From the [NHN documentation](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/doc/GeoBase_nhn_en_Catalogue_1_2.pdf):\n",
    "\n",
    "Permanency code:\n",
    "* -1 unknown\n",
    "* 0 no value available\n",
    "* 1 permanent\n",
    "* 2 intermittent\n",
    "\n",
    "    \n",
    "| water_definition | Label | Code Definition |\n",
    "|------------------|-------|-----------------|\n",
    "| None | 0 | No Waterbody Type value available. |\n",
    "| Canal | 1 | An artificial watercourse serving as a navigable waterway or to channel water. |\n",
    "| Conduit | 2 | An artificial system, such as an Aqueduct, Penstock, Flume, or Sluice, designed to carry water for purposes other than drainage. |\n",
    "| Ditch | 3 | Small, open manmade channel constructed through earth or rock for the purpose of conveying water. |\n",
    "| *Lake | 4 | An inland body of water of considerable area. |\n",
    "| *Reservoir | 5 | A wholly or partially manmade feature for storing and/or regulating and controlling water. |\n",
    "| Watercourse | 6 | A channel on or below the earth's surface through which water may flow. |\n",
    "| Tidal River | 7 | A river in which flow and water surface elevation are affected by the tides. |\n",
    "| *Liquid Waste | 8 | Liquid waste from an industrial complex. |\n",
    "\n",
    "```{warning}\n",
    "The label \"10\" also exists, though I have not found a corresponding definition.  From the image below, it appears they may represent seasonal channels.  Light blue regions are lakes (4) and watercourses (6).\n",
    "```\n",
    "\n",
    "```{figure} img/label_10.png\n",
    "---\n",
    "width: 400px\n",
    "---\n",
    "Darker grey polygons are labeled with the code \"10\" appear to be seasonal channels.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pre-processed lakes polygon file\n",
    "region_lakes_path = os.path.join(base_dir, f'notebooks/data/region_polygons/{region}_lakes.geojson')\n",
    "lakes_df = gpd.read_file(region_lakes_path)\n",
    "lakes_df = lakes_df[[c for c in lakes_df.columns if c not in ['index_right', 'index_left']]]\n",
    "assert lakes_df.crs == ppt_gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Below we apply some subjective criteria to improve the performance of the lake inflow point discovery:\n",
    "1. Remove lakes smaller than 0.01 $km^2$ to speed up the spatial join.\n",
    "2. Only process lakes that contain confluence points in order to relocate points to river mouths.\n",
    "3. Manipulate the lake polygons to smooth the edges -- Where the stream raster disagrees with the NHN polygons it tends to generate spurious inflow points and this step is to mitigate the issue. \n",
    "4. Require a minimum distance to existing confluence points (> 4 pixels).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_df = lakes_df.to_crs(ppt_gdf.crs)  \n",
    "# reproject to projected CRS before calculating area\n",
    "lakes_df['area'] = lakes_df.geometry.area\n",
    "lakes_df['lake_id'] = lakes_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter lakes smaller than 0.01 km^2\n",
    "min_area = 10000\n",
    "lakes_df = lakes_df[lakes_df['area'] > min_area]\n",
    "lakes_df = lakes_df[['acquisition_technique', 'lake_id', 'area', 'water_definition', 'planimetric_accuracy', 'permanency', 'geometry']]\n",
    "# filter by water_definition code \n",
    "# get lakes and reservoirs (4 & 5)\n",
    "lakes_df = lakes_df[(lakes_df['water_definition'] == 4) | (lakes_df['water_definition'] == 5)]\n",
    "lakes_df = lakes_df.dissolve().explode(index_parts=False).reset_index(drop=True)\n",
    "# find and fill holes in polygons\n",
    "lakes_df.geometry = [Polygon(p.exterior) for p in lakes_df.geometry]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify lake geometries\n",
    "\n",
    "Misalignment of the derived stream network and the hydrographic information from the NHN produces spurious points when we try to find streams flowing into lakes.  Simplifying (smoothing) the lake polygons trims long narrow segments classified as lake where feature alignment is most likely to occur.\n",
    "\n",
    "```{figure} img/simplified_polygon.png\n",
    "---\n",
    "width: 400px\n",
    "---\n",
    "A simplified polygon reduces the perimeter of the lake polygon in order to capture where stream lines cross the lake boundary.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_appendages(row):\n",
    "    g = gpd.GeoDataFrame(geometry=[row['geometry']], crs='EPSG:3005')\n",
    "    geom = g.copy().explode(index_parts=True)\n",
    "    geom['area'] = geom.geometry.area\n",
    "    if len(geom) > 1:\n",
    "        # return only the largest geometry by area\n",
    "        largest_geom = geom.loc[geom['area'].idxmax(), 'geometry']\n",
    "        return largest_geom\n",
    "    return row['geometry']\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the set of lakes that contain confluence points\n",
    "lakes_with_pts = gpd.sjoin(lakes_df, ppt_gdf, how='left', predicate='intersects')\n",
    "# the rows with index_right == nan are lake polygons containing no points\n",
    "lakes_with_pts = lakes_with_pts[~lakes_with_pts['index_right'].isna()]\n",
    "# drop all duplicate geometries\n",
    "lakes_with_pts = lakes_with_pts[~lakes_with_pts.index.duplicated(keep='first')]\n",
    "lakes_with_pts['area'] = lakes_with_pts.geometry.area\n",
    "\n",
    "# use negative and positive buffers to smooth the polygon\n",
    "distance = 100  # metres\n",
    "lakes_with_pts.geometry = lakes_with_pts.buffer(-distance).buffer(distance * 1.5).simplify(dx)\n",
    "# drop empty geometries\n",
    "lakes_with_pts = lakes_with_pts[~lakes_with_pts.geometry.is_empty].copy()\n",
    "lakes_with_pts['geometry'] = lakes_with_pts.apply(lambda row: trim_appendages(row), axis=1)\n",
    "\n",
    "lake_cols = ['acquisition_technique', 'lake_id', 'area', \n",
    "             'water_definition', 'planimetric_accuracy', 'permanency', 'geometry']\n",
    "lakes_with_pts = lakes_with_pts[lake_cols]\n",
    "# remove empty geometries\n",
    "lakes_with_pts = lakes_with_pts[~lakes_with_pts.geometry.is_empty]\n",
    "lakes_with_pts.to_file(os.path.join(base_dir, f'notebooks/data/region_polygons/{region}_lakes_simplified.geojson'))\n",
    "\n",
    "n_lakes = len(lakes_with_pts)\n",
    "print(f'    {n_lakes} water body objects in {region} polygon.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_ppts = gpd.sjoin(ppt_gdf, lakes_with_pts, how='left', predicate='within')\n",
    "filtered_ppts = lake_ppts[lake_ppts['index_right'].isna()]\n",
    "print(f'    {len(filtered_ppts)}/{len(ppt_df)} confluence points are not in lakes ({len(ppt_df) - len(filtered_ppts)} points removed).')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and add lake inflows\n",
    "\n",
    "We'll only check lakes that have spurious confluences, the general idea is we shift the in-lake confluence to the inflow location.  The method works best for large lake polygons and relatively smooth geometries where the stream network and NHN features align well, but it adds unnecessary points in other locations.  A few examples of good and bad behaviour are shown below.  \n",
    "\n",
    "```{figure} img/lake_points_removed.png\n",
    "---\n",
    "width: 600px\n",
    "---\n",
    "Confluence points within lakes have been removed, while river mouths have been added.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link_ids(target):\n",
    "    \"\"\"\n",
    "    Traverse the pixels bounding a watershed to find intersecting streams.\n",
    "    If the target cell is on a stream pixel, return the point and the link id.\n",
    "    Link IDs are tracked to have stream links uniquely represented. \n",
    "    If the target cell is not in the stream pixel, we search all neighboring\n",
    "    cells in a 3x3 window for cells in the stream network.\n",
    "    The list of points are interpolated along the boundary line \n",
    "    and not all lake-stream confluences will be captured.    \n",
    "    \"\"\"\n",
    "    x, y, dx, dy, lake, stream_raster = target\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # we have to open in each thread to avoid GIL\n",
    "    # raster = rxr.open_rasterio(stream_link_path, mask=lake, lock=False)\n",
    "    \n",
    "    W = stream_raster.rio.clip_box(x-dx, y-dy, x+dx, y+dy)\n",
    "    t1 = time.time()\n",
    "    # print(f'time to open and clip: {t1-t0:.1f}s')\n",
    "\n",
    "    # stream_links = dill.loads(serialized_W)  # Deserialize the object\n",
    "    # W = stream_links.rio.clip_box(x-dx, y-dy, x+dx, y+dy)\n",
    "    if np.isnan(W.data).all():\n",
    "        return None\n",
    "    \n",
    "    stream_loc = W.sel(x=x, y=y).squeeze()\n",
    "    link_id = stream_loc.compute().item()\n",
    "    if ~np.isnan(link_id):\n",
    "        return (Point(x, y), link_id)\n",
    "    else:\n",
    "        raster_nonzero = W.where(W > 0, drop=True)\n",
    "        # Check surrounding cells for nonzero link_ids\n",
    "        xs, ys = raster_nonzero.x.values, raster_nonzero.y.values\n",
    "        for x1, y1 in zip(xs, ys):\n",
    "            link_id = W.sel(x=x1, y=y1).squeeze().item()\n",
    "            pt = Point(x1, y1)\n",
    "            if ~np.isnan(link_id) & (not lake.contains(pt)):\n",
    "                return (Point(x1, y1), link_id)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search along the boundary for lake-river confluences\n",
    "\n",
    "This step takes **18 minutes** to process on a six core intel i7-8850H @2.6 GHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "tot_pts = 0\n",
    "crs = stream.rio.crs.to_epsg()\n",
    "t0 = time.time()\n",
    "points_to_check = []\n",
    "\n",
    "for _, row in lakes_with_pts.iterrows():\n",
    "\n",
    "    lake_geom = row['geometry']\n",
    "    # resample the shoreline vector to prevent missing confluence points\n",
    "    resampled_shoreline = redistribute_vertices(lake_geom.exterior, dx).coords.xy\n",
    "    xs = np.array(resampled_shoreline[0].tolist())\n",
    "    ys = np.array(resampled_shoreline[1].tolist())\n",
    "\n",
    "    # Find the cells along the interpolated lake polygon boundary\n",
    "    px_pts = stream_links.sel(x=xs, y=ys, method='nearest', tolerance=dx)\n",
    "    coords = list(zip(px_pts['x'].values, px_pts['y'].values))\n",
    "    \n",
    "    stream_raster = rxr.open_rasterio(stream_link_path, mask=lake_geom, lock=False)\n",
    "    inputs = [(x, y, dx, dy, lake_geom, stream_raster) for x, y in coords]\n",
    "    \n",
    "    p = mp.Pool()\n",
    "    results = p.map(find_link_ids, inputs)\n",
    "    p.close()\n",
    "\n",
    "    # drop non stream vals\n",
    "    points_to_check += [r for r in results if r[1] > 0]\n",
    "    \n",
    "    if n % 10 == 0:\n",
    "        t1 = time.time()\n",
    "        print(f'   Processing lake {n}/{len(lakes_with_pts)}.')\n",
    "        print(f'      calculated nearest non-nan of {len(xs)} coordinates in {t1-t0:.1f}s ')\n",
    "    \n",
    "    n += 1\n",
    "\n",
    "t1 = time.time()\n",
    "ttot = (t1-t0) / 60\n",
    "print(f'completed in {ttot:.1f} minutes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_df = pd.DataFrame(points_to_check, columns=['geometry', 'link_id'])\n",
    "pt_df = pt_df.drop_duplicates(subset='link_id', keep='first')\n",
    "gdf = gpd.GeoDataFrame(pt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that found points are not too close to an existing point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "all_pts = []\n",
    "for i, row in gdf.iterrows():\n",
    "    n += 1\n",
    "    if n % 250 == 0:\n",
    "        print(f'{n}/{len(points_to_check)} points checked.')\n",
    "    \n",
    "    # index_right is the lake id the point is contained in\n",
    "    # don't let adjacent points both be pour points\n",
    "    # but avoid measuring distance to points within lakes\n",
    "    nearest_neighbour = ppt_gdf.distance(row['geometry']).min()\n",
    "\n",
    "    # check the point is not within some distance (in m) of an existing point\n",
    "    # 200m is roughly 8 pixels\n",
    "    min_spacing = 200\n",
    "    \n",
    "    if nearest_neighbour > min_spacing:\n",
    "        # check if the potential point is in any of the lakes\n",
    "        all_pts.append(row['geometry'])        \n",
    "        \n",
    "new_pts = gpd.GeoDataFrame(geometry=all_pts, crs=f'EPSG:{crs}')\n",
    "final_ppts = gpd.GeoDataFrame(pd.concat([filtered_ppts, new_pts], axis=0), crs=f'EPSG:{crs}')\n",
    "final_ppts = final_ppts[['cell_idx',\t'acc', 'OUTLET', 'CONF', 'ix',\t'jx', 'geometry']]\n",
    "final_ppts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the river mouth points into a geodataframe and append it to the filtered set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pts = gpd.GeoDataFrame(geometry=all_pts_filtered, crs=f'EPSG:{stream_crs}')\n",
    "# pour_points = gpd.GeoDataFrame(pd.concat([filtered_ppts, new_pts], axis=0), crs=f'EPSG:{stream_crs}')\n",
    "new_pts.to_file(os.path.join(base_dir, f'notebooks/data/pour_points/{region}_pour_points_filtered1.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
